[
  {
    "objectID": "get-started.html",
    "href": "get-started.html",
    "title": "Get Started",
    "section": "",
    "text": "This guide walks you through the complete pybiber workflow, from installing the package to extracting linguistic features from your text corpus."
  },
  {
    "objectID": "get-started.html#overview",
    "href": "get-started.html#overview",
    "title": "Get Started",
    "section": "Overview",
    "text": "Overview\nProcessing a corpus with pybiber involves four main steps:\n\nPrepare your corpus - Organize texts in the required DataFrame format\nInitialize a spaCy model - Load a model with POS tagging and dependency parsing\nParse the corpus - Extract token-level linguistic annotations\nExtract features - Aggregate tokens into document-level feature counts\n\nAfter generating the document-feature matrix, you can proceed to advanced analyses like classification tasks (Reinhart et al. 2024) or Multi-Dimensional Analysis (Biber 1985). See the Biber Analyzer documentation for statistical analysis workflows."
  },
  {
    "objectID": "get-started.html#prerequisites",
    "href": "get-started.html#prerequisites",
    "title": "Get Started",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nInstallation\nInstall pybiber from PyPI:\npip install pybiber\n\n\nspaCy Model\nInstall a spaCy model with part-of-speech tagging and dependency parsing:\npython -m spacy download en_core_web_sm\n\n\n\n\n\n\nModel Requirements\n\n\n\nThe pybiber package requires a spaCy model that performs both part-of-speech tagging and dependency parsing. Most en_core_* models meet these requirements. For other languages, check the spaCy models page."
  },
  {
    "objectID": "get-started.html#step-1-preparing-a-corpus",
    "href": "get-started.html#step-1-preparing-a-corpus",
    "title": "Get Started",
    "section": "Step 1: Preparing a Corpus",
    "text": "Step 1: Preparing a Corpus\n\nImport Libraries\n\nimport spacy\nimport pybiber as pb\nimport polars as pl\n\n\n\nData Structure Requirements\nThe pybiber workflow expects a Polars DataFrame with two essential columns: - doc_id: Unique identifier for each document - text: Raw text content\nThis structure follows conventions established by readtext and quanteda in R.\n\n\nOption 1: Using Sample Data\nFor this tutorial, we’ll use the included sample dataset:\n\nfrom pybiber.data import micusp_mini\n\nLet’s examine the structure:\n\nprint(f\"Corpus shape: {micusp_mini.shape}\")\nmicusp_mini.head()\n\nCorpus shape: (170, 2)\n\n\n\nshape: (5, 2)\n\n\n\ndoc_id\ntext\n\n\nstr\nstr\n\n\n\n\n\"BIO_G0_02_1\"\n\"Ernst Mayr once wrote, \"sympat…\n\n\n\"BIO_G0_03_1\"\n\"The ability of a species to co…\n\n\n\"BIO_G0_06_1\"\n\"Generally, females make a larg…\n\n\n\"BIO_G0_12_1\"\n\"In the field of plant biology,…\n\n\n\"BIO_G0_21_1\"\n\"Parasites in nonhuman animals …\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe micusp_mini dataset is a subset of the Michigan Corpus of Upper-Level Student Papers, containing academic texts from various disciplines. Document IDs encode discipline information (e.g., BIO=Biology, ENG=English).\n\n\n\n\nOption 2: Loading Your Own Data\n\nFrom CSV/Parquet Files\n\n# From CSV\ncorpus = pl.read_csv(\"my_corpus.csv\")\n\n# From Parquet (recommended for large datasets)\ncorpus = pl.read_parquet(\"my_corpus.parquet\")\n\n# From Hugging Face datasets\ncorpus = pl.read_parquet(\n    'hf://datasets/browndw/human-ai-parallel-corpus-mini/hape_mini-text.parquet'\n)\n\n\n\nFrom Text Files in Directory\nUse corpus_from_folder to read all .txt files from a directory:\n\n# Read all .txt files from a directory\ncorpus = pb.corpus_from_folder(\"path/to/text/files\")\n\n# For nested directory structures\ntext_paths = pb.get_text_paths(\"path/to/corpus\", recursive=True)\ncorpus = pb.readtext(text_paths)\n\n\n\nCustom Corpus Creation\n\n# Create corpus from custom data\nimport polars as pl\n\ncorpus = pl.DataFrame({\n    \"doc_id\": [\"doc1\", \"doc2\", \"doc3\"],\n    \"text\": [\n        \"This is the first document.\",\n        \"Here is another text sample.\",\n        \"And this is the third document.\"\n    ]\n})"
  },
  {
    "objectID": "get-started.html#step-2-initialize-spacy-model",
    "href": "get-started.html#step-2-initialize-spacy-model",
    "title": "Get Started",
    "section": "Step 2: Initialize spaCy Model",
    "text": "Step 2: Initialize spaCy Model\nLoad a spaCy model with the required linguistic components:\n\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n\n\nModel Configuration Options\n\n# Option 1: Keep all components (slower but complete)\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Option 2: Disable unnecessary components for speed (recommended)\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n\n# Option 3: Maximize speed (disable more components)\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"lemmatizer\"])\n\n\n\n\n\n\n\nPerformance Tip\n\n\n\nDisabling Named Entity Recognition (ner) typically provides the best speed/functionality balance for feature extraction, as NER isn’t required for Biber features. This is also the default setting in PybiberPipeline."
  },
  {
    "objectID": "get-started.html#step-3-parse-the-text-data",
    "href": "get-started.html#step-3-parse-the-text-data",
    "title": "Get Started",
    "section": "Step 3: Parse the Text Data",
    "text": "Step 3: Parse the Text Data\n\nUsing CorpusProcessor\nThe CorpusProcessor provides efficient, configurable text processing:\n\nprocessor = pb.CorpusProcessor()\ndf_tokens = processor.process_corpus(micusp_mini, nlp_model=nlp)\n\nPerformance: Corpus processing completed in 49.87s\n\n\nThe processing time depends on corpus size and system specifications. For the micusp_mini corpus (~50 documents), expect processing to take about 60 seconds.\n\n\nUnderstanding the Token Output\nThe processor returns a token-level DataFrame with linguistic annotations:\n\nprint(f\"Token DataFrame shape: {df_tokens.shape}\")\ndf_tokens.head(10)\n\nToken DataFrame shape: (544570, 9)\n\n\n\nshape: (10, 9)\n\n\n\ndoc_id\nsentence_id\ntoken_id\ntoken\nlemma\npos\ntag\nhead_token_id\ndep_rel\n\n\nstr\nu32\ni64\nstr\nstr\nstr\nstr\ni64\nstr\n\n\n\n\n\"BIO_G0_02_1\"\n1\n0\n\"Ernst\"\n\"Ernst\"\n\"PROPN\"\n\"NNP\"\n1\n\"compound\"\n\n\n\"BIO_G0_02_1\"\n1\n1\n\"Mayr\"\n\"Mayr\"\n\"PROPN\"\n\"NNP\"\n3\n\"nsubj\"\n\n\n\"BIO_G0_02_1\"\n1\n2\n\"once\"\n\"once\"\n\"ADV\"\n\"RB\"\n3\n\"advmod\"\n\n\n\"BIO_G0_02_1\"\n1\n3\n\"wrote\"\n\"write\"\n\"VERB\"\n\"VBD\"\n3\n\"ROOT\"\n\n\n\"BIO_G0_02_1\"\n1\n4\n\",\"\n\",\"\n\"PUNCT\"\n\",\"\n8\n\"punct\"\n\n\n\"BIO_G0_02_1\"\n1\n5\n\"\"\"\n\"\"\"\n\"PUNCT\"\n\"``\"\n8\n\"punct\"\n\n\n\"BIO_G0_02_1\"\n1\n6\n\"sympatric\"\n\"sympatric\"\n\"ADJ\"\n\"JJ\"\n7\n\"amod\"\n\n\n\"BIO_G0_02_1\"\n1\n7\n\"speciation\"\n\"speciation\"\n\"NOUN\"\n\"NN\"\n8\n\"nsubj\"\n\n\n\"BIO_G0_02_1\"\n1\n8\n\"is\"\n\"be\"\n\"AUX\"\n\"VBZ\"\n3\n\"ccomp\"\n\n\n\"BIO_G0_02_1\"\n1\n9\n\"like\"\n\"like\"\n\"ADP\"\n\"IN\"\n8\n\"prep\"\n\n\n\n\n\n\nKey columns include: - doc_id: Document identifier - token: Raw token text\n- lemma: Lemmatized form - pos: Part-of-speech tag (universal) - tag: Fine-grained POS tag - dep_rel: Dependency relation - sent_id: Sentence identifier\n\n\nPerformance Optimization\nYou can customize processing parameters for better performance:\n\nprocessor = pb.CorpusProcessor()\ndf_tokens = processor.process_corpus(\n    corpus, \n    nlp_model=nlp,\n    n_process=4,        # Use multiple CPU cores\n    batch_size=100,     # Optimize batch size\n    show_progress=True  # Display progress bar\n)\n\n\n\n\n\n\n\nBatch Size Guidelines\n\n\n\n\nSmall corpora (&lt;1000 docs): batch_size=50-100\nMedium corpora (1000-10000 docs): batch_size=100-200\n\nLarge corpora (&gt;10000 docs): batch_size=200-500\n\nLarger batch sizes may actually slow processing due to memory constraints."
  },
  {
    "objectID": "get-started.html#step-4-extract-linguistic-features",
    "href": "get-started.html#step-4-extract-linguistic-features",
    "title": "Get Started",
    "section": "Step 4: Extract Linguistic Features",
    "text": "Step 4: Extract Linguistic Features\n\nBasic Feature Extraction\nTransform token-level data into document-level feature counts using biber:\n\ndf_features = pb.biber(df_tokens)\n\n[INFO] Using MATTR for f_43_type_token\n[INFO] All features normalized per 1000 tokens except: f_43_type_token and f_44_mean_word_length\n\n\n\n\nUnderstanding the Feature Matrix\nThe result is a document-feature matrix with 67 linguistic variables:\n\nprint(f\"Feature matrix shape: {df_features.shape}\")\nprint(f\"Features extracted: {df_features.shape[1] - 1}\")  # Minus doc_id column\ndf_features.head()\n\nFeature matrix shape: (170, 68)\nFeatures extracted: 67\n\n\n\nshape: (5, 68)\n\n\n\ndoc_id\nf_01_past_tense\nf_02_perfect_aspect\nf_03_present_tense\nf_04_place_adverbials\n…\nf_63_split_auxiliary\nf_64_phrasal_coordination\nf_65_clausal_coordination\nf_66_neg_synthetic\nf_67_neg_analytic\n\n\nstr\nf64\nf64\nf64\nf64\n…\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"BIO_G0_02_1\"\n11.574886\n9.821115\n61.381971\n2.104525\n…\n4.910558\n6.664328\n4.209049\n1.403016\n2.806033\n\n\n\"BIO_G0_03_1\"\n20.300088\n3.53045\n59.13504\n1.765225\n…\n0.882613\n7.943513\n2.647838\n0.882613\n7.0609\n\n\n\"BIO_G0_06_1\"\n9.480034\n2.585464\n52.5711\n0.861821\n…\n6.320023\n10.054582\n5.458202\n0.574548\n8.905487\n\n\n\"BIO_G0_12_1\"\n36.900369\n2.767528\n23.98524\n1.845018\n…\n2.767528\n0.922509\n1.845018\n1.845018\n5.535055\n\n\n\"BIO_G0_21_1\"\n40.050858\n2.542912\n26.700572\n2.542912\n…\n3.17864\n7.628735\n6.993007\n2.542912\n2.542912\n\n\n\n\n\n\n\n\nFeature Normalization Options\nBy default, features are normalized per 1,000 tokens, except for two features that use different scales:\n\n# Normalized frequencies (default)\ndf_normalized = pb.biber(df_tokens, normalize=True)\n\n# Raw counts\ndf_raw = pb.biber(df_tokens, normalize=False)\n\n\n\n\n\n\n\nFeature Scaling\n\n\n\n\nMost features: Normalized per 1,000 tokens\nf_43_type_token: Type-token ratio (0-1 scale)\n\nf_44_mean_word_length: Average characters per word\n\nThis normalization enables comparison across documents of different lengths.\n\n\n\n\nType-Token Ratio Options\nThe package offers two type-token ratio calculations:\n\n# Moving Average Type-Token Ratio (default, recommended)\ndf_mattr = pb.biber(df_tokens, force_ttr=False)\n\n# Traditional Type-Token Ratio (for specific comparisons)\ndf_ttr = pb.biber(df_tokens, force_ttr=True)\n\n\n\n\n\n\n\nTTR vs MATTR\n\n\n\n\nMATTR (default): More robust, calculated using 100-token windows\nTraditional TTR: Simple unique tokens / total tokens ratio\nUse consistent measures when comparing corpora processed separately"
  },
  {
    "objectID": "get-started.html#alternative-workflow-high-level-pipeline",
    "href": "get-started.html#alternative-workflow-high-level-pipeline",
    "title": "Get Started",
    "section": "Alternative Workflow: High-Level Pipeline",
    "text": "Alternative Workflow: High-Level Pipeline\nFor streamlined processing, use the PybiberPipeline:\n\nComplete Pipeline Example\n\n# Initialize pipeline with optimal settings\npipeline = pb.PybiberPipeline(\n    model=\"en_core_web_sm\",\n    disable_ner=True,\n    n_process=4,\n    batch_size=100\n)\n\n# Process folder of text files\ndf_features = pipeline.run_from_folder(\"/path/to/texts\", recursive=True)\n\n# Or process existing corpus DataFrame\ndf_features = pipeline.run(micusp_mini)\n\n\n\nPipeline with Token Retention\nIf you need both features and token-level data:\n\n# Return both features and tokens\nfeatures, tokens = pipeline.run(\n    micusp_mini, \n    return_tokens=True,\n    normalize=True\n)"
  },
  {
    "objectID": "get-started.html#data-quality-and-validation",
    "href": "get-started.html#data-quality-and-validation",
    "title": "Get Started",
    "section": "Data Quality and Validation",
    "text": "Data Quality and Validation\n\nExamining Feature Distributions\nBefore analysis, examine your feature distributions:\n\n# Summary statistics\nfeature_columns = df_features.select(pl.selectors.numeric())\nsummary = feature_columns.describe()\nprint(summary)\n\nshape: (9, 68)\n┌─────────┬─────────┬─────────┬─────────┬─────────┬───┬────────┬────────┬────────┬────────┬────────┐\n│ statist ┆ f_01_pa ┆ f_02_pe ┆ f_03_pr ┆ f_04_pl ┆ … ┆ f_63_s ┆ f_64_p ┆ f_65_c ┆ f_66_n ┆ f_67_n │\n│ ic      ┆ st_tens ┆ rfect_a ┆ esent_t ┆ ace_adv ┆   ┆ plit_a ┆ hrasal ┆ lausal ┆ eg_syn ┆ eg_ana │\n│ ---     ┆ e       ┆ spect   ┆ ense    ┆ erbials ┆   ┆ uxilia ┆ _coord ┆ _coord ┆ thetic ┆ lytic  │\n│ str     ┆ ---     ┆ ---     ┆ ---     ┆ ---     ┆   ┆ ry     ┆ inatio ┆ inatio ┆ ---    ┆ ---    │\n│         ┆ f64     ┆ f64     ┆ f64     ┆ f64     ┆   ┆ ---    ┆ n      ┆ n      ┆ f64    ┆ f64    │\n│         ┆         ┆         ┆         ┆         ┆   ┆ f64    ┆ ---    ┆ ---    ┆        ┆        │\n│         ┆         ┆         ┆         ┆         ┆   ┆        ┆ f64    ┆ f64    ┆        ┆        │\n╞═════════╪═════════╪═════════╪═════════╪═════════╪═══╪════════╪════════╪════════╪════════╪════════╡\n│ count   ┆ 170.0   ┆ 170.0   ┆ 170.0   ┆ 170.0   ┆ … ┆ 170.0  ┆ 170.0  ┆ 170.0  ┆ 170.0  ┆ 170.0  │\n│ null_co ┆ 0.0     ┆ 0.0     ┆ 0.0     ┆ 0.0     ┆ … ┆ 0.0    ┆ 0.0    ┆ 0.0    ┆ 0.0    ┆ 0.0    │\n│ unt     ┆         ┆         ┆         ┆         ┆   ┆        ┆        ┆        ┆        ┆        │\n│ mean    ┆ 19.8328 ┆ 4.35336 ┆ 48.3201 ┆ 2.22213 ┆ … ┆ 3.7340 ┆ 9.1199 ┆ 4.9840 ┆ 1.0312 ┆ 5.8196 │\n│         ┆ 55      ┆ 5       ┆ 44      ┆ 3       ┆   ┆ 87     ┆ 05     ┆ 01     ┆ 4      ┆ 86     │\n│ std     ┆ 16.8183 ┆ 2.89646 ┆ 18.7263 ┆ 1.87932 ┆ … ┆ 1.9150 ┆ 4.3323 ┆ 2.6707 ┆ 1.1123 ┆ 2.9414 │\n│         ┆ 59      ┆ 5       ┆ 94      ┆ 4       ┆   ┆ 47     ┆ 36     ┆ 22     ┆ 41     ┆ 46     │\n│ min     ┆ 0.0     ┆ 0.0     ┆ 0.0     ┆ 0.0     ┆ … ┆ 0.4775 ┆ 0.9225 ┆ 0.0    ┆ 0.0    ┆ 0.0    │\n│         ┆         ┆         ┆         ┆         ┆   ┆ 55     ┆ 09     ┆        ┆        ┆        │\n│ 25%     ┆ 6.87521 ┆ 2.23813 ┆ 35.4150 ┆ 0.92776 ┆ … ┆ 2.2594 ┆ 5.8593 ┆ 3.2523 ┆ 0.1579 ┆ 3.7582 │\n│         ┆ 5       ┆ 8       ┆ 8       ┆ 7       ┆   ┆ 61     ┆ 75     ┆ 44     ┆ 03     ┆ 21     │\n│ 50%     ┆ 15.4215 ┆ 3.82690 ┆ 50.4201 ┆ 1.76782 ┆ … ┆ 3.5211 ┆ 8.3688 ┆ 4.7780 ┆ 0.8358 ┆ 5.3412 │\n│         ┆ 22      ┆ 6       ┆ 68      ┆ 6       ┆   ┆ 27     ┆ 62     ┆ 21     ┆ 41     ┆ 46     │\n│ 75%     ┆ 27.5049 ┆ 5.80671 ┆ 61.6792 ┆ 2.91439 ┆ … ┆ 4.9455 ┆ 11.562 ┆ 6.2350 ┆ 1.4030 ┆ 7.3333 │\n│         ┆ 12      ┆ 9       ┆ 68      ┆         ┆   ┆ 98     ┆ 998    ┆ 12     ┆ 16     ┆ 33     │\n│ max     ┆ 82.7814 ┆ 14.8514 ┆ 98.5626 ┆ 12.8314 ┆ … ┆ 10.030 ┆ 25.821 ┆ 16.734 ┆ 8.4859 ┆ 14.806 │\n│         ┆ 57      ┆ 85      ┆ 28      ┆ 8       ┆   ┆ 09     ┆ 596    ┆ 143    ┆ 31     ┆ 378    │\n└─────────┴─────────┴─────────┴─────────┴─────────┴───┴────────┴────────┴────────┴────────┴────────┘\n\n\n\n\nIdentifying Potential Issues\n\n# Check for zero-variance features\nzero_var_features = (\n    feature_columns\n    .std()\n    .transpose(include_header=True)\n    .filter(pl.col(\"column_0\") == 0.0)\n)\n\nif zero_var_features.height &gt; 0:\n    print(\"Zero-variance features found:\")\n    print(zero_var_features)\nelse:\n    print(\"No zero-variance features detected\")\n\nNo zero-variance features detected\n\n\n\n\nDocument Length Analysis\n\n# Analyze document lengths from tokens\ndoc_lengths = (\n    df_tokens\n    .group_by(\"doc_id\")\n    .len()\n    .sort(\"len\", descending=True)\n)\n\nprint(\"Document length distribution:\")\nprint(doc_lengths.describe())"
  },
  {
    "objectID": "get-started.html#next-steps",
    "href": "get-started.html#next-steps",
    "title": "Get Started",
    "section": "Next Steps",
    "text": "Next Steps\nNow that you have extracted linguistic features, you can proceed to:\n\nStatistical Analysis\n\nBiber Analyzer: Multi-Dimensional Analysis and PCA\nTutorials: Advanced workflows and applications\n\n\n\nFeature Understanding\n\nFeature Categories: Complete descriptions of all 67 features\n\n\n\nFurther Processing\n\n# Convert to pandas if needed\ndf_pandas = df_features.to_pandas()\n\n# Export for external analysis\ndf_features.write_csv(\"biber_features.csv\")\ndf_features.write_parquet(\"biber_features.parquet\")"
  },
  {
    "objectID": "get-started.html#troubleshooting",
    "href": "get-started.html#troubleshooting",
    "title": "Get Started",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nCommon Issues\nMemory errors with large corpora: - Reduce batch_size parameter - Process in smaller chunks using the tutorials - Use n_process=1 to reduce memory overhead\nSlow processing: - Increase n_process (up to CPU core count) - Disable unnecessary spaCy components - Use optimal batch_size for your system\nModel loading errors: - Verify spaCy model installation: python -c \"import spacy; spacy.load('en_core_web_sm')\" - Reinstall model if needed: python -m spacy download en_core_web_sm\nFeature extraction errors: - Ensure corpus has required columns (doc_id, text)\n- Check for empty documents in your corpus - Validate text encoding (should be UTF-8)\n\n\nGetting Help\n\nDocumentation: Browse the Reference section\nGitHub Issues: Report bugs or request features\nCommunity: Check existing issues for similar problems"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials and Advanced Usage",
    "section": "",
    "text": "This page provides comprehensive tutorials for advanced pybiber workflows, from corpus preparation to statistical analysis and visualization."
  },
  {
    "objectID": "tutorials.html#tutorial-1-building-and-processing-large-corpora",
    "href": "tutorials.html#tutorial-1-building-and-processing-large-corpora",
    "title": "Tutorials and Advanced Usage",
    "section": "Tutorial 1: Building and Processing Large Corpora",
    "text": "Tutorial 1: Building and Processing Large Corpora\n\nWorking with Directory Structures\nWhen working with large corpora, organizing your texts in a systematic directory structure is crucial:\ncorpus/\n├── academic/\n│   ├── biology/\n│   │   ├── paper001.txt\n│   │   └── paper002.txt\n│   └── literature/\n│       ├── essay001.txt\n│       └── essay002.txt\n└── news/\n    ├── politics/\n    └── sports/\n\n\nRecursive Text Processing\nUse the pipeline to process nested directories:\n\nimport pybiber as pb\n\n# Initialize pipeline with optimized settings\npipeline = pb.PybiberPipeline(\n    model=\"en_core_web_sm\",\n    disable_ner=True,  # Faster processing\n    n_process=4,       # Use multiple cores\n    batch_size=100     # Optimize batch size\n)\n\n# Process entire directory structure\nfeatures = pipeline.run_from_folder(\n    \"corpus/\", \n    recursive=True,\n    normalize=True\n)\n\n\n\nHandling Different Text Formats\nWhile pybiber primarily works with .txt files, you can preprocess other formats:\n\nimport polars as pl\nfrom pathlib import Path\n\n# Process CSV with text columns\ncsv_data = pl.read_csv(\"articles.csv\")\ncorpus = csv_data.select([\n    pl.col(\"article_id\").alias(\"doc_id\"),\n    pl.col(\"content\").alias(\"text\")\n])\n\n# Process with pipeline\npipeline = pb.PybiberPipeline()\nfeatures = pipeline.run(corpus)"
  },
  {
    "objectID": "tutorials.html#tutorial-2-corpus-comparison-and-classification",
    "href": "tutorials.html#tutorial-2-corpus-comparison-and-classification",
    "title": "Tutorials and Advanced Usage",
    "section": "Tutorial 2: Corpus Comparison and Classification",
    "text": "Tutorial 2: Corpus Comparison and Classification\n\nComparing Multiple Corpora\n\nimport pybiber as pb\nimport polars as pl\n\n# Load multiple corpora\nacademic_corpus = pb.corpus_from_folder(\"academic_texts/\")\nnews_corpus = pb.corpus_from_folder(\"news_texts/\")\n\n# Add corpus labels\nacademic_corpus = academic_corpus.with_columns(\n    pl.lit(\"academic\").alias(\"corpus_type\")\n)\nnews_corpus = news_corpus.with_columns(\n    pl.lit(\"news\").alias(\"corpus_type\")\n)\n\n# Combine and process\ncombined_corpus = pl.concat([academic_corpus, news_corpus])\npipeline = pb.PybiberPipeline()\nfeatures = pipeline.run(combined_corpus)\n\n# Extract corpus labels for analysis\nfeatures = features.with_columns(\n    combined_corpus.select(\"corpus_type\")\n)\n\n\n\nFeature-Based Classification\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Prepare data for classification\nX = features.select(pl.selectors.numeric()).to_numpy()\ny = features.get_column(\"corpus_type\").to_numpy()\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))"
  },
  {
    "objectID": "tutorials.html#tutorial-3-advanced-multi-dimensional-analysis",
    "href": "tutorials.html#tutorial-3-advanced-multi-dimensional-analysis",
    "title": "Tutorials and Advanced Usage",
    "section": "Tutorial 3: Advanced Multi-Dimensional Analysis",
    "text": "Tutorial 3: Advanced Multi-Dimensional Analysis\n\nCustom MDA Workflows\n\nimport pybiber as pb\nimport polars as pl\nfrom pybiber.data import micusp_mini\n\n# Process sample data\npipeline = pb.PybiberPipeline()\nfeatures = pipeline.run(micusp_mini)\n\n# Extract discipline information\nfeatures = features.with_columns(\n    pl.col(\"doc_id\").str.extract(r\"^([A-Z]+)\", 0).alias(\"discipline\")\n)\n\n# Initialize analyzer\nanalyzer = pb.BiberAnalyzer(features, id_column=True)\n\n[INFO] Using MATTR for f_43_type_token\n[INFO] All features normalized per 1000 tokens except: f_43_type_token and f_44_mean_word_length\n\n\nPerformance: Corpus processing completed in 74.85s\n\n\n\n\nCustomizing Factor Analysis Parameters\n\n# Experiment with different correlation thresholds\nanalyzer.mda(n_factors=4, cor_min=0.3, threshold=0.4)\n\n# Examine the effect on feature selection\nprint(\"Features after correlation filtering:\")\nprint(analyzer.mda_summary.shape)\n\nINFO:pybiber.biber_analyzer:Dropping 11 variable(s) with max |r| &lt;= 0.30: ['f_04_place_adverbials', 'f_05_time_adverbials', 'f_15_gerunds', 'f_18_by_passives', 'f_25_present_participle', 'f_34_sentence_relatives', 'f_35_because', 'f_46_downtoners', 'f_50_discourse_particles', 'f_53_modal_necessity', 'f_64_phrasal_coordination']\n\n\nFeatures after correlation filtering:\n(4, 6)\n\n\n\n\nComparing Factor Solutions\n\nimport matplotlib.pyplot as plt\n\n# Compare different numbers of factors\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\nfor i, n_factors in enumerate([2, 3, 4, 5]):\n    analyzer.mda(n_factors=n_factors)\n    ax = axes[i // 2, i % 2]\n    \n    # Plot scree plot for each solution\n    analyzer.mdaviz_screeplot()\n    plt.sca(ax)\n    ax.set_title(f\"{n_factors} Factor Solution\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "tutorials.html#tutorial-4-temporal-and-diachronic-analysis",
    "href": "tutorials.html#tutorial-4-temporal-and-diachronic-analysis",
    "title": "Tutorials and Advanced Usage",
    "section": "Tutorial 4: Temporal and Diachronic Analysis",
    "text": "Tutorial 4: Temporal and Diachronic Analysis\n\nAnalyzing Language Change Over Time\n\nimport polars as pl\n\n# Prepare time-stamped corpus\ncorpus = pl.read_csv(\"historical_texts.csv\")\ncorpus = corpus.with_columns([\n    pl.col(\"year\").cast(pl.Int32),\n    pl.col(\"decade\").cast(pl.String)\n])\n\n# Process with pybiber\npipeline = pb.PybiberPipeline()\nfeatures = pipeline.run(corpus)\n\n# Add temporal metadata\nfeatures = features.join(\n    corpus.select([\"doc_id\", \"year\", \"decade\"]),\n    on=\"doc_id\"\n)\n\n# Analyze by decade\ndecade_analysis = pb.BiberAnalyzer(\n    features.drop(\"year\"), \n    id_column=True\n)\n\n# Examine temporal dimensions\ndecade_analysis.mda(n_factors=3)\ndecade_analysis.mdaviz_groupmeans(factor=1)\n\n\n\nTrend Analysis\n\n# Calculate feature trends over time\ntrends = (\n    features\n    .group_by(\"decade\")\n    .agg([\n        pl.selectors.numeric().mean().suffix(\"_mean\"),\n        pl.selectors.numeric().std().suffix(\"_std\")\n    ])\n    .sort(\"decade\")\n)\n\n# Visualize specific feature trends\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(trends[\"decade\"], trends[\"f_01_past_tense_mean\"], \n         marker='o', label='Past Tense')\nplt.plot(trends[\"decade\"], trends[\"f_03_present_tense_mean\"], \n         marker='s', label='Present Tense')\nplt.xlabel(\"Decade\")\nplt.ylabel(\"Normalized Frequency\")\nplt.legend()\nplt.title(\"Tense Usage Over Time\")\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "tutorials.html#tutorial-5-cross-linguistic-and-multilingual-analysis",
    "href": "tutorials.html#tutorial-5-cross-linguistic-and-multilingual-analysis",
    "title": "Tutorials and Advanced Usage",
    "section": "Tutorial 5: Cross-Linguistic and Multilingual Analysis",
    "text": "Tutorial 5: Cross-Linguistic and Multilingual Analysis\n\nComparing Languages\n\n# Process different language corpora\nenglish_pipeline = pb.PybiberPipeline(model=\"en_core_web_sm\")\nspanish_pipeline = pb.PybiberPipeline(model=\"es_core_news_sm\")\n\nenglish_features = english_pipeline.run_from_folder(\"english_texts/\")\nspanish_features = spanish_pipeline.run_from_folder(\"spanish_texts/\")\n\n# Add language labels\nenglish_features = english_features.with_columns(\n    pl.lit(\"English\").alias(\"language\")\n)\nspanish_features = spanish_features.with_columns(\n    pl.lit(\"Spanish\").alias(\"language\")\n)\n\n# Combine for comparative analysis\nmultilingual_features = pl.concat([english_features, spanish_features])\n\n\n\nLanguage-Specific Adaptations\n\n# Customize feature extraction for specific languages\ndef extract_language_specific_features(tokens, language=\"en\"):\n    base_features = pb.biber(tokens, normalize=True)\n    \n    if language == \"es\":\n        # Add Spanish-specific features\n        spanish_features = extract_spanish_subjunctive(tokens)\n        base_features = base_features.join(spanish_features, on=\"doc_id\")\n    \n    return base_features"
  },
  {
    "objectID": "tutorials.html#tutorial-6-statistical-validation-and-robustness",
    "href": "tutorials.html#tutorial-6-statistical-validation-and-robustness",
    "title": "Tutorials and Advanced Usage",
    "section": "Tutorial 6: Statistical Validation and Robustness",
    "text": "Tutorial 6: Statistical Validation and Robustness\n\nCross-Validation of Factor Solutions\n\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\ndef validate_factor_solution(features, n_factors=3, n_splits=5):\n    \"\"\"Cross-validate factor stability.\"\"\"\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    loadings_stability = []\n    \n    for train_idx, _ in kf.split(features):\n        # Sample training data\n        train_features = features[train_idx]\n        \n        # Fit MDA\n        analyzer = pb.BiberAnalyzer(train_features)\n        analyzer.mda(n_factors=n_factors)\n        \n        # Store loadings\n        loadings_stability.append(analyzer.mda_loadings)\n    \n    # Calculate loading stability metrics\n    return analyze_loading_stability(loadings_stability)\n\n\n\nBootstrap Confidence Intervals\n\ndef bootstrap_mda_confidence(features, n_bootstrap=1000):\n    \"\"\"Calculate bootstrap confidence intervals for factor loadings.\"\"\"\n    bootstrap_loadings = []\n    n_docs = features.shape[0]\n    \n    for i in range(n_bootstrap):\n        # Resample with replacement\n        sample_idx = np.random.choice(n_docs, n_docs, replace=True)\n        boot_features = features[sample_idx]\n        \n        # Fit MDA\n        analyzer = pb.BiberAnalyzer(boot_features)\n        analyzer.mda(n_factors=3)\n        bootstrap_loadings.append(analyzer.mda_loadings)\n    \n    # Calculate confidence intervals\n    return calculate_loading_confidence(bootstrap_loadings)"
  },
  {
    "objectID": "tutorials.html#tutorial-7-performance-optimization",
    "href": "tutorials.html#tutorial-7-performance-optimization",
    "title": "Tutorials and Advanced Usage",
    "section": "Tutorial 7: Performance Optimization",
    "text": "Tutorial 7: Performance Optimization\n\nMemory-Efficient Processing\n\n# Process large corpora in chunks\ndef process_large_corpus(corpus_path, chunk_size=1000):\n    \"\"\"Process large corpus in memory-efficient chunks.\"\"\"\n    \n    # Get all text files\n    text_files = list(Path(corpus_path).rglob(\"*.txt\"))\n    all_features = []\n    \n    # Process in chunks\n    for i in range(0, len(text_files), chunk_size):\n        chunk_files = text_files[i:i + chunk_size]\n        \n        # Create temporary corpus\n        chunk_corpus = pb.readtext(chunk_files)\n        \n        # Process chunk\n        pipeline = pb.PybiberPipeline()\n        chunk_features = pipeline.run(chunk_corpus)\n        \n        all_features.append(chunk_features)\n        \n        # Clear memory\n        del chunk_corpus, chunk_features\n    \n    # Combine all features\n    return pl.concat(all_features)\n\n\n\nParallel Processing Optimization\n\n# Optimize parallel processing parameters\ndef find_optimal_batch_size(corpus, model=\"en_core_web_sm\"):\n    \"\"\"Find optimal batch size for your system.\"\"\"\n    import time\n    \n    batch_sizes = [10, 50, 100, 200, 500]\n    processing_times = []\n    \n    for batch_size in batch_sizes:\n        pipeline = pb.PybiberPipeline(\n            model=model,\n            batch_size=batch_size,\n            n_process=4\n        )\n        \n        start_time = time.time()\n        _ = pipeline.run(corpus.head(1000))  # Test subset\n        end_time = time.time()\n        \n        processing_times.append(end_time - start_time)\n    \n    # Find optimal batch size\n    optimal_idx = np.argmin(processing_times)\n    return batch_sizes[optimal_idx]"
  },
  {
    "objectID": "tutorials.html#best-practices-summary",
    "href": "tutorials.html#best-practices-summary",
    "title": "Tutorials and Advanced Usage",
    "section": "Best Practices Summary",
    "text": "Best Practices Summary\n\nData Preparation\n\nOrganize texts in systematic directory structures\nEncode metadata in filenames or separate files\nClean text appropriately for your spaCy model\nValidate corpus structure before processing\n\n\n\nFeature Extraction\n\nChoose appropriate spaCy models for your texts\nConsider disabling unnecessary components (like NER) for speed\nUse appropriate normalization (per 1000 tokens vs. absolute counts)\nMonitor memory usage with large corpora\n\n\n\nStatistical Analysis\n\nExamine scree plots before selecting number of factors\nValidate factor solutions with multiple approaches\nConsider cross-validation for robust results\nDocument all analytical decisions and parameters\n\n\n\nPerformance Optimization\n\nExperiment with batch sizes and parallel processing\nProcess large corpora in chunks if memory is limited\nMonitor system resources during processing\nCache intermediate results when possible\n\nThese tutorials provide a foundation for advanced pybiber usage. Adapt these patterns to your specific research questions and computational constraints."
  },
  {
    "objectID": "reference/mdaviz_screeplot.html",
    "href": "reference/mdaviz_screeplot.html",
    "title": "mdaviz_screeplot",
    "section": "",
    "text": "BiberAnalyzer.mdaviz_screeplot(width=6, height=3, dpi=150, mda=True)\nGenerate a scree plot for determining factors.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwidth\n\nThe width of the plot.\n6\n\n\nheight\n\nThe height of the plot.\n3\n\n\ndpi\n\nThe resolution of the plot.\n150\n\n\nmda\n\nWhether or not non-colinear features should be filter out per Biber’s multi-dimensional analysis procedure.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFigure\nA matplotlib figure.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`mdaviz_screeplot`"
    ]
  },
  {
    "objectID": "reference/mdaviz_screeplot.html#parameters",
    "href": "reference/mdaviz_screeplot.html#parameters",
    "title": "mdaviz_screeplot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nwidth\n\nThe width of the plot.\n6\n\n\nheight\n\nThe height of the plot.\n3\n\n\ndpi\n\nThe resolution of the plot.\n150\n\n\nmda\n\nWhether or not non-colinear features should be filter out per Biber’s multi-dimensional analysis procedure.\nTrue",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`mdaviz_screeplot`"
    ]
  },
  {
    "objectID": "reference/mdaviz_screeplot.html#returns",
    "href": "reference/mdaviz_screeplot.html#returns",
    "title": "mdaviz_screeplot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nFigure\nA matplotlib figure.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`mdaviz_screeplot`"
    ]
  },
  {
    "objectID": "reference/mda_biber.html",
    "href": "reference/mda_biber.html",
    "title": "mda_biber",
    "section": "",
    "text": "BiberAnalyzer.mda_biber(threshold=0.35)\nProject results onto Biber’s dimensions.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nthreshold\nfloat\nThe factor loading threshold (in absolute value) used to calculate dimension scores.\n0.35",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`mda_biber`"
    ]
  },
  {
    "objectID": "reference/mda_biber.html#parameters",
    "href": "reference/mda_biber.html#parameters",
    "title": "mda_biber",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nthreshold\nfloat\nThe factor loading threshold (in absolute value) used to calculate dimension scores.\n0.35",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`mda_biber`"
    ]
  },
  {
    "objectID": "reference/run_biber.html",
    "href": "reference/run_biber.html",
    "title": "run_biber",
    "section": "",
    "text": "run_biber\npipeline.run_biber(\n    corpus,\n    nlp=None,\n    model='en_core_web_sm',\n    disable_ner=True,\n    n_process=CONFIG.DEFAULT_N_PROCESS,\n    batch_size=CONFIG.DEFAULT_BATCH_SIZE,\n    return_tokens=False,\n    normalize=True,\n    force_ttr=False,\n)\nOne-liner: parse -&gt; biber() from an in-memory corpus DataFrame.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`run_biber`"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "Read in and prepare data\n\n\n\nCorpusProcessor\nMain class that orchestrates corpus processing pipeline.\n\n\ncorpus_from_folder\nImport all text files from a directory.\n\n\nget_text_paths\nGet a list of full paths for all text files.\n\n\nreadtext\nImport all text files from a list of paths.\n\n\nspacy_parse\nParse a corpus (legacy public API).\n\n\nget_noun_phrases\nExtract expanded noun phrases using the ‘en_core_web_sm’ model.\n\n\n\n\n\n\nHigh-level orchestration wrappers\n\n\n\nPybiberPipeline\nEnd-to-end convenience wrapper for common pybiber workflows.\n\n\nrun_biber_from_folder\nOne-liner: read -&gt; parse -&gt; biber() from a folder of .txt files.\n\n\nrun_biber\nOne-liner: parse -&gt; biber() from an in-memory corpus DataFrame.\n\n\n\n\n\n\nGenerate a biber document-feature matrix\n\n\n\nbiber\nExtract Biber features from a parsed corpus.\n\n\n\n\n\n\nAnalyze a biber document-feature matrix\n\n\n\nmda\nExecute Biber’s multi-dimensional anlaysis.\n\n\nmda_biber\nProject results onto Biber’s dimensions.\n\n\npca\nExecute principal component analysis.\n\n\nmdaviz_screeplot\nGenerate a scree plot for determining factors.\n\n\nmdaviz_groupmeans\nGenerate a stick plot of the group means for a factor.\n\n\npcaviz_groupmeans\nGenerate a scatter plot of the group means along 2 components.\n\n\npcaviz_contrib\nGenerate a bar plot of variable contributions to a component."
  },
  {
    "objectID": "reference/index.html#pybiber-utility-functions",
    "href": "reference/index.html#pybiber-utility-functions",
    "title": "Reference",
    "section": "",
    "text": "Read in and prepare data\n\n\n\nCorpusProcessor\nMain class that orchestrates corpus processing pipeline.\n\n\ncorpus_from_folder\nImport all text files from a directory.\n\n\nget_text_paths\nGet a list of full paths for all text files.\n\n\nreadtext\nImport all text files from a list of paths.\n\n\nspacy_parse\nParse a corpus (legacy public API).\n\n\nget_noun_phrases\nExtract expanded noun phrases using the ‘en_core_web_sm’ model."
  },
  {
    "objectID": "reference/index.html#pybiber-pipeline",
    "href": "reference/index.html#pybiber-pipeline",
    "title": "Reference",
    "section": "",
    "text": "High-level orchestration wrappers\n\n\n\nPybiberPipeline\nEnd-to-end convenience wrapper for common pybiber workflows.\n\n\nrun_biber_from_folder\nOne-liner: read -&gt; parse -&gt; biber() from a folder of .txt files.\n\n\nrun_biber\nOne-liner: parse -&gt; biber() from an in-memory corpus DataFrame."
  },
  {
    "objectID": "reference/index.html#pybiber-parse",
    "href": "reference/index.html#pybiber-parse",
    "title": "Reference",
    "section": "",
    "text": "Generate a biber document-feature matrix\n\n\n\nbiber\nExtract Biber features from a parsed corpus."
  },
  {
    "objectID": "reference/index.html#pybiber-methods",
    "href": "reference/index.html#pybiber-methods",
    "title": "Reference",
    "section": "",
    "text": "Analyze a biber document-feature matrix\n\n\n\nmda\nExecute Biber’s multi-dimensional anlaysis.\n\n\nmda_biber\nProject results onto Biber’s dimensions.\n\n\npca\nExecute principal component analysis.\n\n\nmdaviz_screeplot\nGenerate a scree plot for determining factors.\n\n\nmdaviz_groupmeans\nGenerate a stick plot of the group means for a factor.\n\n\npcaviz_groupmeans\nGenerate a scatter plot of the group means along 2 components.\n\n\npcaviz_contrib\nGenerate a bar plot of variable contributions to a component."
  },
  {
    "objectID": "reference/biber.html",
    "href": "reference/biber.html",
    "title": "biber",
    "section": "",
    "text": "parse_functions.biber(tokens, normalize=True, force_ttr=False)\nExtract Biber features from a parsed corpus.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntokens\npl.DataFrame\nA polars DataFrame with the output of the spacy_parse function.\nrequired\n\n\nnormalize\nOptional[bool]\nNormalize counts per 1000 tokens.\nTrue\n\n\nforce_ttr\nOptional[bool]\nForce the calcuation of type-token ratio rather than moving average type-token ratio.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npl.DataFrame\nA polars DataFrame with, counts of feature frequencies.\n\n\n\n\n\n\nMATTR is the default as it is less sensitive than TTR to variations in text lenghth. However, the function will automatically use TTR if any of the corpus texts are less than 200 words. Thus, forcing TTR can be necessary when processing multiple corpora that you want to be consistent.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`biber`"
    ]
  },
  {
    "objectID": "reference/biber.html#parameters",
    "href": "reference/biber.html#parameters",
    "title": "biber",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntokens\npl.DataFrame\nA polars DataFrame with the output of the spacy_parse function.\nrequired\n\n\nnormalize\nOptional[bool]\nNormalize counts per 1000 tokens.\nTrue\n\n\nforce_ttr\nOptional[bool]\nForce the calcuation of type-token ratio rather than moving average type-token ratio.\nFalse",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`biber`"
    ]
  },
  {
    "objectID": "reference/biber.html#returns",
    "href": "reference/biber.html#returns",
    "title": "biber",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npl.DataFrame\nA polars DataFrame with, counts of feature frequencies.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`biber`"
    ]
  },
  {
    "objectID": "reference/biber.html#notes",
    "href": "reference/biber.html#notes",
    "title": "biber",
    "section": "",
    "text": "MATTR is the default as it is less sensitive than TTR to variations in text lenghth. However, the function will automatically use TTR if any of the corpus texts are less than 200 words. Thus, forcing TTR can be necessary when processing multiple corpora that you want to be consistent.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`biber`"
    ]
  },
  {
    "objectID": "reference/readtext.html",
    "href": "reference/readtext.html",
    "title": "readtext",
    "section": "",
    "text": "parse_utils.readtext(paths)\nImport all text files from a list of paths.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npaths\nList\nA list of paths of text files returned by get_text_paths.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList\nA list of full paths.\n\n\n\n\n\n\nModeled on the R function readtext.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`readtext`"
    ]
  },
  {
    "objectID": "reference/readtext.html#parameters",
    "href": "reference/readtext.html#parameters",
    "title": "readtext",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npaths\nList\nA list of paths of text files returned by get_text_paths.\nrequired",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`readtext`"
    ]
  },
  {
    "objectID": "reference/readtext.html#returns",
    "href": "reference/readtext.html#returns",
    "title": "readtext",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nList\nA list of full paths.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`readtext`"
    ]
  },
  {
    "objectID": "reference/readtext.html#notes",
    "href": "reference/readtext.html#notes",
    "title": "readtext",
    "section": "",
    "text": "Modeled on the R function readtext.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`readtext`"
    ]
  },
  {
    "objectID": "reference/corpus_from_folder.html",
    "href": "reference/corpus_from_folder.html",
    "title": "corpus_from_folder",
    "section": "",
    "text": "parse_utils.corpus_from_folder(directory)\nImport all text files from a directory.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\nstr\nA directory containing text files.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npl.DataFrame\nA polars DataFrame.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`corpus_from_folder`"
    ]
  },
  {
    "objectID": "reference/corpus_from_folder.html#parameters",
    "href": "reference/corpus_from_folder.html#parameters",
    "title": "corpus_from_folder",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndirectory\nstr\nA directory containing text files.\nrequired",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`corpus_from_folder`"
    ]
  },
  {
    "objectID": "reference/corpus_from_folder.html#returns",
    "href": "reference/corpus_from_folder.html#returns",
    "title": "corpus_from_folder",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npl.DataFrame\nA polars DataFrame.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`corpus_from_folder`"
    ]
  },
  {
    "objectID": "reference/run_biber_from_folder.html",
    "href": "reference/run_biber_from_folder.html",
    "title": "run_biber_from_folder",
    "section": "",
    "text": "run_biber_from_folder\npipeline.run_biber_from_folder(\n    directory,\n    model='en_core_web_sm',\n    disable_ner=True,\n    n_process=CONFIG.DEFAULT_N_PROCESS,\n    batch_size=CONFIG.DEFAULT_BATCH_SIZE,\n    recursive=False,\n    return_tokens=False,\n    normalize=True,\n    force_ttr=False,\n)\nOne-liner: read -&gt; parse -&gt; biber() from a folder of .txt files.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`run_biber_from_folder`"
    ]
  },
  {
    "objectID": "reference/PybiberPipeline.html",
    "href": "reference/PybiberPipeline.html",
    "title": "PybiberPipeline",
    "section": "",
    "text": "pipeline.PybiberPipeline(\n    nlp=None,\n    model='en_core_web_sm',\n    disable_ner=True,\n    n_process=CONFIG.DEFAULT_N_PROCESS,\n    batch_size=CONFIG.DEFAULT_BATCH_SIZE,\n    show_progress=None,\n)\nEnd-to-end convenience wrapper for common pybiber workflows.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnlp\nOptional[Language]\nPre-loaded spaCy model. If None, the model named by model will be loaded lazily on first use.\nNone\n\n\nmodel\nstr\nName of the spaCy model to load when nlp is None. Defaults to “en_core_web_sm”.\n'en_core_web_sm'\n\n\ndisable_ner\nbool\nWhen True, disable spaCy’s NER component for speed and stability. Parser is still enabled and required. Defaults to True.\nTrue\n\n\nn_process\nint\nNumber of processes to use for spaCy’s pipe.\nCONFIG.DEFAULT_N_PROCESS\n\n\nbatch_size\nint\nBatch size for spaCy’s pipe.\nCONFIG.DEFAULT_BATCH_SIZE\n\n\nshow_progress\nOptional[bool]\nWhether to show internal progress indicators when processing. If None, it is determined based on corpus size.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfeatures\nCompute Biber features from token-level parses.\n\n\nfrom_folder\nRead .txt files from a folder into a corpus DataFrame.\n\n\nparse\nParse a corpus with spaCy using the configured settings.\n\n\nrun\nParse and compute features from an in-memory corpus DataFrame.\n\n\nrun_from_folder\nRead, parse, and compute features from a folder of .txt files.\n\n\nto_analyzer\nCreate a BiberAnalyzer from a Biber feature matrix.\n\n\n\n\n\npipeline.PybiberPipeline.features(tokens, normalize=True, force_ttr=False)\nCompute Biber features from token-level parses.\n\n\n\npipeline.PybiberPipeline.from_folder(directory, recursive=False)\nRead .txt files from a folder into a corpus DataFrame.\n\n\n\npipeline.PybiberPipeline.parse(corpus)\nParse a corpus with spaCy using the configured settings.\n\n\n\npipeline.PybiberPipeline.run(\n    corpus,\n    return_tokens=False,\n    normalize=True,\n    force_ttr=False,\n)\nParse and compute features from an in-memory corpus DataFrame.\n\n\n\npipeline.PybiberPipeline.run_from_folder(\n    directory,\n    recursive=False,\n    return_tokens=False,\n    normalize=True,\n    force_ttr=False,\n)\nRead, parse, and compute features from a folder of .txt files.\n\n\n\npipeline.PybiberPipeline.to_analyzer(biber_df)\nCreate a BiberAnalyzer from a Biber feature matrix.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`PybiberPipeline`"
    ]
  },
  {
    "objectID": "reference/PybiberPipeline.html#parameters",
    "href": "reference/PybiberPipeline.html#parameters",
    "title": "PybiberPipeline",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nnlp\nOptional[Language]\nPre-loaded spaCy model. If None, the model named by model will be loaded lazily on first use.\nNone\n\n\nmodel\nstr\nName of the spaCy model to load when nlp is None. Defaults to “en_core_web_sm”.\n'en_core_web_sm'\n\n\ndisable_ner\nbool\nWhen True, disable spaCy’s NER component for speed and stability. Parser is still enabled and required. Defaults to True.\nTrue\n\n\nn_process\nint\nNumber of processes to use for spaCy’s pipe.\nCONFIG.DEFAULT_N_PROCESS\n\n\nbatch_size\nint\nBatch size for spaCy’s pipe.\nCONFIG.DEFAULT_BATCH_SIZE\n\n\nshow_progress\nOptional[bool]\nWhether to show internal progress indicators when processing. If None, it is determined based on corpus size.\nNone",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`PybiberPipeline`"
    ]
  },
  {
    "objectID": "reference/PybiberPipeline.html#methods",
    "href": "reference/PybiberPipeline.html#methods",
    "title": "PybiberPipeline",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfeatures\nCompute Biber features from token-level parses.\n\n\nfrom_folder\nRead .txt files from a folder into a corpus DataFrame.\n\n\nparse\nParse a corpus with spaCy using the configured settings.\n\n\nrun\nParse and compute features from an in-memory corpus DataFrame.\n\n\nrun_from_folder\nRead, parse, and compute features from a folder of .txt files.\n\n\nto_analyzer\nCreate a BiberAnalyzer from a Biber feature matrix.\n\n\n\n\n\npipeline.PybiberPipeline.features(tokens, normalize=True, force_ttr=False)\nCompute Biber features from token-level parses.\n\n\n\npipeline.PybiberPipeline.from_folder(directory, recursive=False)\nRead .txt files from a folder into a corpus DataFrame.\n\n\n\npipeline.PybiberPipeline.parse(corpus)\nParse a corpus with spaCy using the configured settings.\n\n\n\npipeline.PybiberPipeline.run(\n    corpus,\n    return_tokens=False,\n    normalize=True,\n    force_ttr=False,\n)\nParse and compute features from an in-memory corpus DataFrame.\n\n\n\npipeline.PybiberPipeline.run_from_folder(\n    directory,\n    recursive=False,\n    return_tokens=False,\n    normalize=True,\n    force_ttr=False,\n)\nRead, parse, and compute features from a folder of .txt files.\n\n\n\npipeline.PybiberPipeline.to_analyzer(biber_df)\nCreate a BiberAnalyzer from a Biber feature matrix.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`PybiberPipeline`"
    ]
  },
  {
    "objectID": "feature-categories.html",
    "href": "feature-categories.html",
    "title": "Feature categories",
    "section": "",
    "text": "Feature\nDescription\n\n\n\n\n-\nA. Tense and aspect markers\n\n\nf_01_past_tense\nPast tense\n\n\nf_02_perfect_aspect\nPerfect aspect\n\n\nf_03_present_tense\nPresent tense\n\n\n-\nB. Place and time adverbials\n\n\nf_04_place_adverbials\nPlace adverbials (e.g., above, beside, outdoors)\n\n\nf_05_time_adverbials\nTime adverbials (e.g., early, instantly, soon)\n\n\n-\nC. Pronouns and pro-verbs\n\n\nf_06_first_person_pronouns\nFirst-person pronouns\n\n\nf_07_second_person_pronouns\nSecond-person pronouns\n\n\nf_08_third_person_pronouns\nThird-person personal pronouns (excluding it)\n\n\nf_09_pronoun_it\nPronoun it\n\n\nf_10_demonstrative_pronoun\nDemonstrative pronouns (that, this, these, those as pronouns)\n\n\nf_11_indefinite_pronoun\nIndefinite pronounes (e.g., anybody, nothing, someone)\n\n\nf_12_proverb_do\nPro-verb do\n\n\n-\nD. Questions\n\n\nf_13_wh_question\nDirect wh-questions\n\n\n-\nE. Nominal forms\n\n\nf_14_nominalization\nNominalizations (ending in -tion, -ment, -ness, -ity)\n\n\nf_15_gerunds\nGerunds (participial forms functioning as nouns)\n\n\nf_16_other_nouns\nTotal other nouns\n\n\n-\nF. Passives\n\n\nf_17_agentless_passives\nAgentless passives\n\n\nf_18_by_passives\nby-passives\n\n\n-\nG. Stative forms\n\n\nf_19_be_main_verb\nbe as main verb\n\n\nf_20_existential_there\nExistential there\n\n\n-\nH. Subordination features\n\n\nf_21_that_verb_comp\nthat verb complements (e.g., I said [that he went].)\n\n\nf_22_that_adj_comp\nthat adjective complements (e.g., I’m glad [that you like it].)\n\n\nf_23_wh_clause\nwh-clauses (e.g., I believed [what he told me].)\n\n\nf_24_infinitives\nInfinitives\n\n\nf_25_present_participle\nPresent participial adverbial clauses (e.g., [Stuffing his mouth with cookies], Joe ran out the door.)\n\n\nf_26_past_participle\nPast participial adverbial clauses (e.g., [Built in a single week], the house would stand for fifty years.)\n\n\nf_27_past_participle_whiz\nPast participial postnominal (reduced relative) clauses (e.g., the solution [produced by this process])\n\n\nf_28_present_participle_whiz\nPresent participial postnominal (reduced relative) clauses (e.g., the event [causing this decline[)\n\n\nf_29_that_subj\nthat relative clauses on subject position (e.g., the dog [that bit me])\n\n\nf_30_that_obj\nthat relative clauses on object position (e.g., the dog [that I saw])\n\n\nf_31_wh_subj\nwh- relatives on subject position (e.g., the man [who likes popcorn])\n\n\nf_32_wh_obj\nwh- relatives on object position (e.g., the man [who Sally likes])\n\n\nf_33_pied_piping\nPied-piping relative clauses (e.g., the manner [in which he was told])\n\n\nf_34_sentence_relatives\nSentence relatives (e.g., Bob likes fried mangoes, [which is the most disgusting thing I’ve ever heard of].)\n\n\nf_35_because\nCausative adverbial subordinator (because)\n\n\nf_36_though\nConcessive adverbial subordinators (although, though)\n\n\nf_37_if\nConditional adverbial subordinators (if, unless)\n\n\nf_38_other_adv_sub\nOther adverbial subordinators (e.g., since, while, whereas)\n\n\n-\nI. Prepositional phrases, adjectives and adverbs\n\n\nf_39_prepositions\nTotal prepositional phrases\n\n\nf_40_adj_attr\nAttributive adjectives (e.g., the [big] horse)\n\n\nf_41_adj_pred\nPredicative adjectives (e.g., The horse is [big].)\n\n\nf_42_adverbs\nTotal adverbs\n\n\n-\nJ. Lexical specificity\n\n\nf_43_type_token\nType-token ratio (including punctuation)\n\n\nf_44_mean_word_length\nAverage word length (across tokens, excluding punctuation)\n\n\n-\nK. Lexical classes\n\n\nf_45_conjuncts\nConjuncts (e.g., consequently, furthermore, however)\n\n\nf_46_downtoners\nDowntoners (e.g., barely, nearly, slightly)\n\n\nf_47_hedges\nHedges (e.g., at about, something like, almost)\n\n\nf_48_amplifiers\nAmplifiers (e.g., absolutely, extremely, perfectly)\n\n\nf_49_emphatics\nEmphatics (e.g., a lot, for sure, really)\n\n\nf_50_discourse_particles\nDiscourse particles (e.g., sentence-initial well, now, anyway)\n\n\nf_51_demonstratives\nDemonstratives\n\n\n-\nL. Modals\n\n\nf_52_modal_possibility\nPossibility modals (can, may, might, could)\n\n\nf_53_modal_necessity\nNecessity modals (ought, should, must)\n\n\nf_54_modal_predictive\nPredictive modals (will, would, shall)\n\n\n-\nM. Specialized verb classes\n\n\nf_55_verb_public\nPublic verbs (e.g., assert, declare, mention)\n\n\nf_56_verb_private\nPrivate verbs (e.g., assume, believe, doubt, know)\n\n\nf_57_verb_suasive\nSuasive verbs (e.g., command, insist, propose)\n\n\nf_58_verb_seem\nseem and appear\n\n\n-\nN. Reduced forms and dispreferred structures\n\n\nf_59_contractions\nContractions\n\n\nf_60_that_deletion\nSubordinator that deletion (e.g., I think [he went].)\n\n\nf_61_stranded_preposition\nStranded prepositions (e.g., the candidate that I was thinking [of])\n\n\nf_62_split_infinitve\nSplit infinitives (e.g., He wants [to convincingly prove] that …)\n\n\nf_63_split_auxiliary\nSplit auxiliaries (e.g., They [were apparently shown] to …)\n\n\n-\nO. Co-ordination\n\n\nf_64_phrasal_coordination\nPhrasal co-ordination (N and N; Adj and Adj; V and V; Adv and Adv)\n\n\nf_65_clausal_coordination\nIndependent clause co-ordination (clause-initial and)\n\n\n-\nP. Negation\n\n\nf_66_neg_synthetic\nSynthetic negation (e.g., No answer is good enough for Jones.)\n\n\nf_67_neg_analytic\nAnalytic negation (e.g., That isn’t good enough.)"
  },
  {
    "objectID": "reference/get_text_paths.html",
    "href": "reference/get_text_paths.html",
    "title": "get_text_paths",
    "section": "",
    "text": "parse_utils.get_text_paths(directory, recursive=False)\nGet a list of full paths for all text files.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\nstr\nA string indictating a path to a directory.\nrequired\n\n\nrecursive\n\nWhether to search subdirectories.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList\nA list of full paths.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`get_text_paths`"
    ]
  },
  {
    "objectID": "reference/get_text_paths.html#parameters",
    "href": "reference/get_text_paths.html#parameters",
    "title": "get_text_paths",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndirectory\nstr\nA string indictating a path to a directory.\nrequired\n\n\nrecursive\n\nWhether to search subdirectories.\nFalse",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`get_text_paths`"
    ]
  },
  {
    "objectID": "reference/get_text_paths.html#returns",
    "href": "reference/get_text_paths.html#returns",
    "title": "get_text_paths",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nList\nA list of full paths.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`get_text_paths`"
    ]
  },
  {
    "objectID": "reference/pcaviz_groupmeans.html",
    "href": "reference/pcaviz_groupmeans.html",
    "title": "pcaviz_groupmeans",
    "section": "",
    "text": "BiberAnalyzer.pcaviz_groupmeans(pc=1, width=8, height=4, dpi=150)\nGenerate a scatter plot of the group means along 2 components.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npc\n\nThe principal component for the x-axis.\n1\n\n\nwidth\n\nThe width of the plot.\n8\n\n\nheight\n\nThe height of the plot.\n4\n\n\ndpi\n\nThe resolution of the plot.\n150\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFigure\nA matplotlib figure.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`pcaviz_groupmeans`"
    ]
  },
  {
    "objectID": "reference/pcaviz_groupmeans.html#parameters",
    "href": "reference/pcaviz_groupmeans.html#parameters",
    "title": "pcaviz_groupmeans",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npc\n\nThe principal component for the x-axis.\n1\n\n\nwidth\n\nThe width of the plot.\n8\n\n\nheight\n\nThe height of the plot.\n4\n\n\ndpi\n\nThe resolution of the plot.\n150",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`pcaviz_groupmeans`"
    ]
  },
  {
    "objectID": "reference/pcaviz_groupmeans.html#returns",
    "href": "reference/pcaviz_groupmeans.html#returns",
    "title": "pcaviz_groupmeans",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nFigure\nA matplotlib figure.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`pcaviz_groupmeans`"
    ]
  },
  {
    "objectID": "reference/mda.html",
    "href": "reference/mda.html",
    "title": "mda",
    "section": "",
    "text": "BiberAnalyzer.mda(n_factors=3, cor_min=0.2, threshold=0.35)\nExecute Biber’s multi-dimensional anlaysis.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn_factors\nint\nThe number of factors to extract.\n3\n\n\ncor_min\nfloat\nThe minimum correlation at which to drop variables.\n0.2\n\n\nthreshold\nfloat\nThe factor loading threshold (in absolute value) used to calculate dimension scores.\n0.35",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`mda`"
    ]
  },
  {
    "objectID": "reference/mda.html#parameters",
    "href": "reference/mda.html#parameters",
    "title": "mda",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn_factors\nint\nThe number of factors to extract.\n3\n\n\ncor_min\nfloat\nThe minimum correlation at which to drop variables.\n0.2\n\n\nthreshold\nfloat\nThe factor loading threshold (in absolute value) used to calculate dimension scores.\n0.35",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`mda`"
    ]
  },
  {
    "objectID": "reference/CorpusProcessor.html",
    "href": "reference/CorpusProcessor.html",
    "title": "CorpusProcessor",
    "section": "",
    "text": "parse_utils.CorpusProcessor()\nMain class that orchestrates corpus processing pipeline.\n\n\n\n\n\nName\nDescription\n\n\n\n\nextract_noun_phrases\nProcess a corpus using the complete pipeline.\n\n\nprocess_corpus\nProcess a corpus using the complete pipeline.\n\n\nspacy_parse\nAlias of process_corpus for parity with legacy naming.\n\n\n\n\n\nparse_utils.CorpusProcessor.extract_noun_phrases(\n    corp,\n    nlp_model,\n    n_process=CONFIG.DEFAULT_N_PROCESS,\n    batch_size=CONFIG.DEFAULT_BATCH_SIZE,\n    disable_ner=True,\n    show_progress=None,\n)\nProcess a corpus using the complete pipeline.\n:param corp: A polars DataFrame containing ‘doc_id’ and ‘text’ columns. :param nlp_model: An ‘en_core_web_sm’ instance. :param n_process: The number of parallel processes to use during parsing. :param batch_size: The batch size to use during parsing. :param show_progress: Whether to show progress for large corpora. If None, will auto-determine based on corpus size. :return: A polars DataFrame with full dependency parses.\n\n\n\nparse_utils.CorpusProcessor.process_corpus(\n    corp,\n    nlp_model,\n    n_process=CONFIG.DEFAULT_N_PROCESS,\n    batch_size=CONFIG.DEFAULT_BATCH_SIZE,\n    disable_ner=True,\n    show_progress=None,\n)\nProcess a corpus using the complete pipeline.\n:param corp: A polars DataFrame containing ‘doc_id’ and ‘text’ columns. :param nlp_model: An ‘en_core_web’ instance. :param n_process: The number of parallel processes to use during parsing. :param batch_size: The batch size to use during parsing. :param show_progress: Whether to show progress for large corpora. If None, will auto-determine based on corpus size. :return: A polars DataFrame with full dependency parses.\n\n\n\nparse_utils.CorpusProcessor.spacy_parse(\n    corp,\n    nlp_model,\n    n_process=CONFIG.DEFAULT_N_PROCESS,\n    batch_size=CONFIG.DEFAULT_BATCH_SIZE,\n    disable_ner=True,\n    show_progress=None,\n)\nAlias of process_corpus for parity with legacy naming.\nProvided to ease migration and satisfy tests that call CorpusProcessor.spacy_parse()."
  },
  {
    "objectID": "reference/CorpusProcessor.html#methods",
    "href": "reference/CorpusProcessor.html#methods",
    "title": "CorpusProcessor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nextract_noun_phrases\nProcess a corpus using the complete pipeline.\n\n\nprocess_corpus\nProcess a corpus using the complete pipeline.\n\n\nspacy_parse\nAlias of process_corpus for parity with legacy naming.\n\n\n\n\n\nparse_utils.CorpusProcessor.extract_noun_phrases(\n    corp,\n    nlp_model,\n    n_process=CONFIG.DEFAULT_N_PROCESS,\n    batch_size=CONFIG.DEFAULT_BATCH_SIZE,\n    disable_ner=True,\n    show_progress=None,\n)\nProcess a corpus using the complete pipeline.\n:param corp: A polars DataFrame containing ‘doc_id’ and ‘text’ columns. :param nlp_model: An ‘en_core_web_sm’ instance. :param n_process: The number of parallel processes to use during parsing. :param batch_size: The batch size to use during parsing. :param show_progress: Whether to show progress for large corpora. If None, will auto-determine based on corpus size. :return: A polars DataFrame with full dependency parses.\n\n\n\nparse_utils.CorpusProcessor.process_corpus(\n    corp,\n    nlp_model,\n    n_process=CONFIG.DEFAULT_N_PROCESS,\n    batch_size=CONFIG.DEFAULT_BATCH_SIZE,\n    disable_ner=True,\n    show_progress=None,\n)\nProcess a corpus using the complete pipeline.\n:param corp: A polars DataFrame containing ‘doc_id’ and ‘text’ columns. :param nlp_model: An ‘en_core_web’ instance. :param n_process: The number of parallel processes to use during parsing. :param batch_size: The batch size to use during parsing. :param show_progress: Whether to show progress for large corpora. If None, will auto-determine based on corpus size. :return: A polars DataFrame with full dependency parses.\n\n\n\nparse_utils.CorpusProcessor.spacy_parse(\n    corp,\n    nlp_model,\n    n_process=CONFIG.DEFAULT_N_PROCESS,\n    batch_size=CONFIG.DEFAULT_BATCH_SIZE,\n    disable_ner=True,\n    show_progress=None,\n)\nAlias of process_corpus for parity with legacy naming.\nProvided to ease migration and satisfy tests that call CorpusProcessor.spacy_parse()."
  },
  {
    "objectID": "reference/spacy_parse.html",
    "href": "reference/spacy_parse.html",
    "title": "spacy_parse",
    "section": "",
    "text": "parse_utils.spacy_parse(\n    corp,\n    nlp_model,\n    n_process=1,\n    batch_size=25,\n    disable_ner=True,\n)\nParse a corpus (legacy public API).\nThis function is maintained for backward compatibility and now delegates to the new class-based pipeline (CorpusProcessor.process_corpus).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncorp\npl.DataFrame\nDataFrame with ‘doc_id’ and ‘text’.\nrequired\n\n\nnlp_model\nLanguage\nspaCy model instance (e.g., ‘en_core_web_sm’).\nrequired\n\n\nn_process\nint\nNumber of processes passed to spaCy’s pipe.\n1\n\n\nbatch_size\nint\nBatch size for spaCy pipe.\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npl.DataFrame\nToken-level dependency parse output identical in schema to the legacy implementation.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`spacy_parse`"
    ]
  },
  {
    "objectID": "reference/spacy_parse.html#parameters",
    "href": "reference/spacy_parse.html#parameters",
    "title": "spacy_parse",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ncorp\npl.DataFrame\nDataFrame with ‘doc_id’ and ‘text’.\nrequired\n\n\nnlp_model\nLanguage\nspaCy model instance (e.g., ‘en_core_web_sm’).\nrequired\n\n\nn_process\nint\nNumber of processes passed to spaCy’s pipe.\n1\n\n\nbatch_size\nint\nBatch size for spaCy pipe.\n25",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`spacy_parse`"
    ]
  },
  {
    "objectID": "reference/spacy_parse.html#returns",
    "href": "reference/spacy_parse.html#returns",
    "title": "spacy_parse",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npl.DataFrame\nToken-level dependency parse output identical in schema to the legacy implementation.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`spacy_parse`"
    ]
  },
  {
    "objectID": "reference/mdaviz_groupmeans.html",
    "href": "reference/mdaviz_groupmeans.html",
    "title": "mdaviz_groupmeans",
    "section": "",
    "text": "BiberAnalyzer.mdaviz_groupmeans(factor=1, width=3, height=7, dpi=150)\nGenerate a stick plot of the group means for a factor.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfactor\n\nThe factor or dimension to plot.\n1\n\n\nwidth\n\nThe width of the plot.\n3\n\n\nheight\n\nThe height of the plot.\n7\n\n\ndpi\n\nThe resolution of the plot.\n150\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFigure\nA matplotlib figure.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`mdaviz_groupmeans`"
    ]
  },
  {
    "objectID": "reference/mdaviz_groupmeans.html#parameters",
    "href": "reference/mdaviz_groupmeans.html#parameters",
    "title": "mdaviz_groupmeans",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfactor\n\nThe factor or dimension to plot.\n1\n\n\nwidth\n\nThe width of the plot.\n3\n\n\nheight\n\nThe height of the plot.\n7\n\n\ndpi\n\nThe resolution of the plot.\n150",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`mdaviz_groupmeans`"
    ]
  },
  {
    "objectID": "reference/mdaviz_groupmeans.html#returns",
    "href": "reference/mdaviz_groupmeans.html#returns",
    "title": "mdaviz_groupmeans",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nFigure\nA matplotlib figure.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`mdaviz_groupmeans`"
    ]
  },
  {
    "objectID": "reference/pcaviz_contrib.html",
    "href": "reference/pcaviz_contrib.html",
    "title": "pcaviz_contrib",
    "section": "",
    "text": "BiberAnalyzer.pcaviz_contrib(pc=1, width=8, height=4, dpi=150)\nGenerate a bar plot of variable contributions to a component.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npc\n\nThe principal component.\n1\n\n\nwidth\n\nThe width of the plot.\n8\n\n\nheight\n\nThe height of the plot.\n4\n\n\ndpi\n\nThe resolution of the plot.\n150\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFigure\nA matplotlib figure.\n\n\n\n\n\n\nModeled on the R function fviz_contrib.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`pcaviz_contrib`"
    ]
  },
  {
    "objectID": "reference/pcaviz_contrib.html#parameters",
    "href": "reference/pcaviz_contrib.html#parameters",
    "title": "pcaviz_contrib",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npc\n\nThe principal component.\n1\n\n\nwidth\n\nThe width of the plot.\n8\n\n\nheight\n\nThe height of the plot.\n4\n\n\ndpi\n\nThe resolution of the plot.\n150",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`pcaviz_contrib`"
    ]
  },
  {
    "objectID": "reference/pcaviz_contrib.html#returns",
    "href": "reference/pcaviz_contrib.html#returns",
    "title": "pcaviz_contrib",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nFigure\nA matplotlib figure.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`pcaviz_contrib`"
    ]
  },
  {
    "objectID": "reference/pcaviz_contrib.html#notes",
    "href": "reference/pcaviz_contrib.html#notes",
    "title": "pcaviz_contrib",
    "section": "",
    "text": "Modeled on the R function fviz_contrib.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`pcaviz_contrib`"
    ]
  },
  {
    "objectID": "reference/get_noun_phrases.html",
    "href": "reference/get_noun_phrases.html",
    "title": "get_noun_phrases",
    "section": "",
    "text": "parse_utils.get_noun_phrases(\n    corp,\n    nlp_model,\n    n_process=1,\n    batch_size=25,\n    disable_ner=True,\n)\nExtract expanded noun phrases using the ‘en_core_web_sm’ model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncorp\npl.DataFrame\nA polars DataFrame conataining a ‘doc_id’ column and a ‘text’ column.\nrequired\n\n\nnlp_model\nLanguage\nAn ‘en_core_web_sm’ instance.\nrequired\n\n\nn_process\n\nThe number of parallel processes to use during parsing.\n1\n\n\nbatch_size\n\nThe batch size to use during parsing.\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npl.DataFrame\na polars DataFrame with, noun phrases and their assocated part-of-speech tags.\n\n\n\n\n\n\nNoun phrases can be extracted directly from the noun_chunks attribute. However, per spaCy’s documentation the attribute does not permit nested noun phrases, for example when a prepositional phrases modifies a preceding noun phrase. This function extracts elatorated noun phrases in their complete form.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`get_noun_phrases`"
    ]
  },
  {
    "objectID": "reference/get_noun_phrases.html#parameters",
    "href": "reference/get_noun_phrases.html#parameters",
    "title": "get_noun_phrases",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ncorp\npl.DataFrame\nA polars DataFrame conataining a ‘doc_id’ column and a ‘text’ column.\nrequired\n\n\nnlp_model\nLanguage\nAn ‘en_core_web_sm’ instance.\nrequired\n\n\nn_process\n\nThe number of parallel processes to use during parsing.\n1\n\n\nbatch_size\n\nThe batch size to use during parsing.\n25",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`get_noun_phrases`"
    ]
  },
  {
    "objectID": "reference/get_noun_phrases.html#returns",
    "href": "reference/get_noun_phrases.html#returns",
    "title": "get_noun_phrases",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npl.DataFrame\na polars DataFrame with, noun phrases and their assocated part-of-speech tags.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`get_noun_phrases`"
    ]
  },
  {
    "objectID": "reference/get_noun_phrases.html#notes",
    "href": "reference/get_noun_phrases.html#notes",
    "title": "get_noun_phrases",
    "section": "",
    "text": "Noun phrases can be extracted directly from the noun_chunks attribute. However, per spaCy’s documentation the attribute does not permit nested noun phrases, for example when a prepositional phrases modifies a preceding noun phrase. This function extracts elatorated noun phrases in their complete form.",
    "crumbs": [
      "Get started",
      "Parsing Data",
      "`get_noun_phrases`"
    ]
  },
  {
    "objectID": "reference/pca.html",
    "href": "reference/pca.html",
    "title": "pca",
    "section": "",
    "text": "BiberAnalyzer.pca()\nExecute principal component analysis.\n\n\nThis is largely a convenience function as most of its outputs are produced by wrappers for sklearn. However, variable contribution is adapted from the FactoMineR function fviz_contrib.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`pca`"
    ]
  },
  {
    "objectID": "reference/pca.html#notes",
    "href": "reference/pca.html#notes",
    "title": "pca",
    "section": "",
    "text": "This is largely a convenience function as most of its outputs are produced by wrappers for sklearn. However, variable contribution is adapted from the FactoMineR function fviz_contrib.",
    "crumbs": [
      "Get started",
      "Analyzing Data",
      "`pca`"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pybiber",
    "section": "",
    "text": "A comprehensive Python package for linguistic feature extraction and Multi-Dimensional Analysis\nThe pybiber package provides tools for extracting 67 lexicogrammatical and functional features described by Biber (1988) and widely used for text-type, register, and genre classification tasks in corpus linguistics."
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "pybiber",
    "section": "Key Features",
    "text": "Key Features\n\nFeature Extraction: Automated extraction of 67 linguistic features from text corpora\nMulti-Dimensional Analysis: Implementation of Biber’s MDA methodology for register analysis\n\nPrincipal Component Analysis: Alternative dimensionality reduction approaches\nVisualization Tools: Comprehensive plotting functions for exploratory data analysis\nHigh Performance: Built on spaCy and Polars for efficient processing\nPipeline Integration: End-to-end workflows from raw text to statistical analysis"
  },
  {
    "objectID": "index.html#applications",
    "href": "index.html#applications",
    "title": "pybiber",
    "section": "Applications",
    "text": "Applications\nThe pybiber package is suitable for:\n\nRegister and genre analysis in corpus linguistics\nText classification and machine learning preprocessing\n\nDiachronic language change studies\nCross-linguistic variation research\nAcademic writing analysis and pedagogical applications\nStylometric analysis and authorship attribution"
  },
  {
    "objectID": "index.html#technical-foundation",
    "href": "index.html#technical-foundation",
    "title": "pybiber",
    "section": "Technical Foundation",
    "text": "Technical Foundation\nThe package uses spaCy part-of-speech tagging and dependency parsing to extract linguistic features. All data processing leverages the Polars DataFrame library for high-performance analytics.\n\n\n\n\n\n\nAccuracy Considerations\n\n\n\nFeature extraction builds from the outputs of probabilistic taggers, so the accuracy of resulting counts depends on the accuracy of those models. Texts with irregular spellings, non-normative punctuation, etc. may produce unreliable outputs unless taggers are specifically tuned for those domains."
  },
  {
    "objectID": "index.html#related-implementations",
    "href": "index.html#related-implementations",
    "title": "pybiber",
    "section": "Related Implementations",
    "text": "Related Implementations\n\npseudobibeR: R implementation of Biber’s feature set\nquanteda: R package for quantitative text analysis"
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "pybiber",
    "section": "Quick Start",
    "text": "Quick Start\nFor users eager to jump in, here’s a minimal example:\n\nimport pybiber as pb\n\n# One-line corpus processing\npipeline = pb.PybiberPipeline()\nfeatures = pipeline.run_from_folder(\"path/to/texts\")"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "pybiber",
    "section": "Installation",
    "text": "Installation\nYou can install the released version of pybiber from PyPI:\npip install pybiber\n\nspaCy Model Installation\nInstall a spaCy model for linguistic analysis:\npython -m spacy download en_core_web_sm\n\n\n\n\n\n\nAlternative Installation Methods\n\n\n\nYou can also install spaCy models directly via pip:\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl\nFor other languages, see the spaCy models documentation."
  },
  {
    "objectID": "index.html#basic-workflow",
    "href": "index.html#basic-workflow",
    "title": "pybiber",
    "section": "Basic Workflow",
    "text": "Basic Workflow\n\nData Requirements\nThe pybiber package works with text corpora structured as DataFrames. The biber function expects a Polars DataFrame with: - doc_id column: Unique identifiers for each document - text column: Raw text content\nThis structure follows conventions established by readtext and quanteda in R.\n\n\nStep-by-Step Processing\n\n1. Import Libraries and Load Data\n\nimport spacy\nimport pybiber as pb\nfrom pybiber.data import micusp_mini\n\nLet’s examine the structure of our sample corpus:\n\nmicusp_mini.head()\n\n\nshape: (5, 2)\n\n\n\ndoc_id\ntext\n\n\nstr\nstr\n\n\n\n\n\"BIO_G0_02_1\"\n\"Ernst Mayr once wrote, \"sympat…\n\n\n\"BIO_G0_03_1\"\n\"The ability of a species to co…\n\n\n\"BIO_G0_06_1\"\n\"Generally, females make a larg…\n\n\n\"BIO_G0_12_1\"\n\"In the field of plant biology,…\n\n\n\"BIO_G0_21_1\"\n\"Parasites in nonhuman animals …\n\n\n\n\n\n\nThe micusp_mini dataset is a subset of the Michigan Corpus of Upper-Level Student Papers, containing academic texts from various disciplines.\n\n\n\n\n\n\nBuilding Your Own Corpus\n\n\n\nTo process your own texts, use corpus_from_folder to read all text files from a directory:\ncorpus = pb.corpus_from_folder(\"path/to/your/texts\")\n\n\n\n\n2. Initialize spaCy Model\nThe pybiber package requires a spaCy model with part-of-speech tagging and dependency parsing capabilities:\n\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n\n\n\n\n\n\n\nNote\n\n\n\nWe disable Named Entity Recognition (ner) to increase processing speed, though this is optional. The essential components for feature extraction are the part-of-speech tagger and dependency parser. Note that PybiberPipeline disables NER by default.\n\n\n\n\n3. Process the Corpus\nUse the CorpusProcessor to parse your texts:\n\nprocessor = pb.CorpusProcessor()\ndf_spacy = processor.process_corpus(micusp_mini, nlp_model=nlp)\n\nPerformance: Corpus processing completed in 49.84s\n\n\nThis returns a token-level DataFrame with linguistic annotations, structured similarly to spacyr output:\n\ndf_spacy.head()\n\n\nshape: (5, 9)\n\n\n\ndoc_id\nsentence_id\ntoken_id\ntoken\nlemma\npos\ntag\nhead_token_id\ndep_rel\n\n\nstr\nu32\ni64\nstr\nstr\nstr\nstr\ni64\nstr\n\n\n\n\n\"BIO_G0_02_1\"\n1\n0\n\"Ernst\"\n\"Ernst\"\n\"PROPN\"\n\"NNP\"\n1\n\"compound\"\n\n\n\"BIO_G0_02_1\"\n1\n1\n\"Mayr\"\n\"Mayr\"\n\"PROPN\"\n\"NNP\"\n3\n\"nsubj\"\n\n\n\"BIO_G0_02_1\"\n1\n2\n\"once\"\n\"once\"\n\"ADV\"\n\"RB\"\n3\n\"advmod\"\n\n\n\"BIO_G0_02_1\"\n1\n3\n\"wrote\"\n\"write\"\n\"VERB\"\n\"VBD\"\n3\n\"ROOT\"\n\n\n\"BIO_G0_02_1\"\n1\n4\n\",\"\n\",\"\n\"PUNCT\"\n\",\"\n8\n\"punct\"\n\n\n\n\n\n\n\n\n4. Extract Linguistic Features\nFinally, aggregate the token-level data into document-level feature counts:\n\ndf_biber = pb.biber(df_spacy)\n\n[INFO] Using MATTR for f_43_type_token\n[INFO] All features normalized per 1000 tokens except: f_43_type_token and f_44_mean_word_length\n\n\nThe resulting document-feature matrix contains 67 linguistic features plus document identifiers:\n\nprint(f\"Matrix dimensions: {df_biber.shape}\")\ndf_biber.head()\n\nMatrix dimensions: (170, 68)\n\n\n\nshape: (5, 68)\n\n\n\ndoc_id\nf_01_past_tense\nf_02_perfect_aspect\nf_03_present_tense\nf_04_place_adverbials\n…\nf_63_split_auxiliary\nf_64_phrasal_coordination\nf_65_clausal_coordination\nf_66_neg_synthetic\nf_67_neg_analytic\n\n\nstr\nf64\nf64\nf64\nf64\n…\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"BIO_G0_02_1\"\n11.574886\n9.821115\n61.381971\n2.104525\n…\n4.910558\n6.664328\n4.209049\n1.403016\n2.806033\n\n\n\"BIO_G0_03_1\"\n20.300088\n3.53045\n59.13504\n1.765225\n…\n0.882613\n7.943513\n2.647838\n0.882613\n7.0609\n\n\n\"BIO_G0_06_1\"\n9.480034\n2.585464\n52.5711\n0.861821\n…\n6.320023\n10.054582\n5.458202\n0.574548\n8.905487\n\n\n\"BIO_G0_12_1\"\n36.900369\n2.767528\n23.98524\n1.845018\n…\n2.767528\n0.922509\n1.845018\n1.845018\n5.535055\n\n\n\"BIO_G0_21_1\"\n40.050858\n2.542912\n26.700572\n2.542912\n…\n3.17864\n7.628735\n6.993007\n2.542912\n2.542912"
  },
  {
    "objectID": "index.html#understanding-the-output",
    "href": "index.html#understanding-the-output",
    "title": "pybiber",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\nFeature Normalization\n\n\n\n\n\n\nFeature Scaling\n\n\n\nBy default, all features are normalized per 1,000 tokens except: - f_43_type_token: Type-token ratio (0-1 scale) - f_44_mean_word_length: Average word length in characters\nSet normalize=False to return absolute counts instead of normalized frequencies.\n\n\n\n\nDocument Metadata\n\n\n\n\n\n\nEncoding Metadata\n\n\n\nEncode important metadata into your document IDs (file names) for downstream analysis. In the micusp_mini data, the first three letters represent academic disciplines (e.g., BIO=Biology, ENG=English)."
  },
  {
    "objectID": "index.html#advanced-usage",
    "href": "index.html#advanced-usage",
    "title": "pybiber",
    "section": "Advanced Usage",
    "text": "Advanced Usage\n\nHigh-Level Pipeline\nFor streamlined processing, use the PybiberPipeline:\n\n# Process a folder of .txt files in one step\npipeline = pb.PybiberPipeline(model=\"en_core_web_sm\")\nfeatures = pipeline.run_from_folder(\"path/to/texts\", recursive=True)\n\n\n\nWorking with Pandas\nAll DataFrames use Polars for performance. To convert to pandas:\n\ndf_pandas = df_biber.to_pandas()  # Requires pandas and pyarrow"
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "pybiber",
    "section": "Next Steps",
    "text": "Next Steps\n\nGet Started Guide: Detailed walkthrough of the basic workflow\nFeature Categories: Complete list and descriptions of all 67 features\nBiber Analyzer: Multi-Dimensional Analysis and statistical visualization tools\n\nFor advanced analytical workflows, explore the BiberAnalyzer class for factor analysis and dimensional visualization."
  },
  {
    "objectID": "biber-analyzer.html",
    "href": "biber-analyzer.html",
    "title": "Biber analyzer",
    "section": "",
    "text": "The BiberAnalyzer class provides a comprehensive toolkit for conducting Multi-Dimensional Analysis (MDA) and Principal Component Analysis (PCA) on linguistic feature data. This implementation follows Biber’s methodology (Biber 1985) for exploring the systematic co-occurrence patterns of linguistic features across text types and registers."
  },
  {
    "objectID": "biber-analyzer.html#overview-of-multi-dimensional-analysis",
    "href": "biber-analyzer.html#overview-of-multi-dimensional-analysis",
    "title": "Biber analyzer",
    "section": "Overview of Multi-Dimensional Analysis",
    "text": "Overview of Multi-Dimensional Analysis\nMulti-Dimensional Analysis is a specific implementation of exploratory factor analysis that has been extensively used in corpus linguistics and register analysis. A representative sample of MDA studies can be seen in the table of contents of a tribute volume.\nThe MDA procedure consists of four main steps:\n\nIdentification of relevant variables - Selection of linguistic features for analysis\nExtraction of factors from variables - Statistical identification of underlying dimensions\nFunctional interpretation of factors as dimensions - Linguistic interpretation of statistical patterns\nPlacement of categories on the dimensions - Positioning of text types along the extracted dimensions\n\nA detailed description of the MDA procedure can be found here."
  },
  {
    "objectID": "biber-analyzer.html#key-features-of-biberanalyzer",
    "href": "biber-analyzer.html#key-features-of-biberanalyzer",
    "title": "Biber analyzer",
    "section": "Key Features of BiberAnalyzer",
    "text": "Key Features of BiberAnalyzer\nThe BiberAnalyzer class offers several analytical and visualization capabilities:\n\nExploratory Data Analysis: Scree plots for factor selection\nMulti-Dimensional Analysis: Factor extraction with Promax rotation\nPrincipal Component Analysis: Alternative dimensionality reduction approach\nVisualization Tools: Comprehensive plotting functions for results interpretation\nBiber Replication: Projection onto Biber’s original dimensions\nStatistical Summaries: Detailed output of loadings, scores, and group means"
  },
  {
    "objectID": "biber-analyzer.html#setting-up-the-analysis",
    "href": "biber-analyzer.html#setting-up-the-analysis",
    "title": "Biber analyzer",
    "section": "Setting Up the Analysis",
    "text": "Setting Up the Analysis\n\nLoading Libraries and Data\nFirst we will import our libraries and load the sample dataset:\n\nimport spacy\nimport pybiber as pb\nimport polars as pl\nfrom pybiber.data import micusp_mini\n\nThe sample data (micusp_mini) is a subset of the Michigan Corpus of Upper-Level Student Papers, containing academic texts from various disciplines. This dataset is ideal for demonstrating cross-disciplinary variation in linguistic features.\n\n\nText Processing Pipeline\nNext, we’ll process the raw texts through the spaCy NLP pipeline to extract linguistic features:\n\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\nprocessor = pb.CorpusProcessor()\ndf_spacy = processor.process_corpus(micusp_mini, nlp_model=nlp)\n\nPerformance: Corpus processing completed in 49.67s\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe explicitly disable Named Entity Recognition (ner) here for consistency with the PybiberPipeline defaults, which disables NER by default for improved processing speed since NER is not required for Biber feature extraction.\n\n\n\n\nFeature Extraction\nNow we extract the 67 Biber linguistic features from the parsed texts:\n\ndf_biber = pb.biber(df_spacy)\n\n[INFO] Using MATTR for f_43_type_token\n[INFO] All features normalized per 1000 tokens except: f_43_type_token and f_44_mean_word_length\n\n\nThe biber() function returns normalized frequencies (per 1,000 tokens) for all features except type-token ratio and mean word length, which use different scales appropriate to their nature."
  },
  {
    "objectID": "biber-analyzer.html#data-preparation-for-analysis",
    "href": "biber-analyzer.html#data-preparation-for-analysis",
    "title": "Biber analyzer",
    "section": "Data Preparation for Analysis",
    "text": "Data Preparation for Analysis\n\nUnderstanding the Dataset Structure\nLet’s examine the structure of our feature matrix:\n\ndf_biber.head()\n\n\nshape: (5, 68)\n\n\n\ndoc_id\nf_01_past_tense\nf_02_perfect_aspect\nf_03_present_tense\nf_04_place_adverbials\n…\nf_63_split_auxiliary\nf_64_phrasal_coordination\nf_65_clausal_coordination\nf_66_neg_synthetic\nf_67_neg_analytic\n\n\nstr\nf64\nf64\nf64\nf64\n…\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"BIO_G0_02_1\"\n11.574886\n9.821115\n61.381971\n2.104525\n…\n4.910558\n6.664328\n4.209049\n1.403016\n2.806033\n\n\n\"BIO_G0_03_1\"\n20.300088\n3.53045\n59.13504\n1.765225\n…\n0.882613\n7.943513\n2.647838\n0.882613\n7.0609\n\n\n\"BIO_G0_06_1\"\n9.480034\n2.585464\n52.5711\n0.861821\n…\n6.320023\n10.054582\n5.458202\n0.574548\n8.905487\n\n\n\"BIO_G0_12_1\"\n36.900369\n2.767528\n23.98524\n1.845018\n…\n2.767528\n0.922509\n1.845018\n1.845018\n5.535055\n\n\n\"BIO_G0_21_1\"\n40.050858\n2.542912\n26.700572\n2.542912\n…\n3.17864\n7.628735\n6.993007\n2.542912\n2.542912\n\n\n\n\n\n\n\n\nCreating Categorical Variables\nFor MDA, we need a categorical grouping variable to compare different text types or registers. In our MICUSP data, discipline information is encoded in the doc_id field.\n\n\n\n\n\n\nNote\n\n\n\nThe MICUSP data are down-sampled from the Michigan Corpus of Upper-Level Student Papers. Each document ID contains a three-letter discipline code (e.g., BIO for Biology, ENG for English, etc.).\n\n\nWe can extract the discipline codes and create a categorical variable:\n\ndf_biber = (\n    df_biber\n    .with_columns(\n        pl.col(\"doc_id\").str.extract(r\"^([A-Z])+\", 0)\n        .alias(\"discipline\")\n      )\n      )\n\ndf_biber.head()\n\n\nshape: (5, 69)\n\n\n\ndoc_id\nf_01_past_tense\nf_02_perfect_aspect\nf_03_present_tense\nf_04_place_adverbials\n…\nf_64_phrasal_coordination\nf_65_clausal_coordination\nf_66_neg_synthetic\nf_67_neg_analytic\ndiscipline\n\n\nstr\nf64\nf64\nf64\nf64\n…\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"BIO_G0_02_1\"\n11.574886\n9.821115\n61.381971\n2.104525\n…\n6.664328\n4.209049\n1.403016\n2.806033\n\"BIO\"\n\n\n\"BIO_G0_03_1\"\n20.300088\n3.53045\n59.13504\n1.765225\n…\n7.943513\n2.647838\n0.882613\n7.0609\n\"BIO\"\n\n\n\"BIO_G0_06_1\"\n9.480034\n2.585464\n52.5711\n0.861821\n…\n10.054582\n5.458202\n0.574548\n8.905487\n\"BIO\"\n\n\n\"BIO_G0_12_1\"\n36.900369\n2.767528\n23.98524\n1.845018\n…\n0.922509\n1.845018\n1.845018\n5.535055\n\"BIO\"\n\n\n\"BIO_G0_21_1\"\n40.050858\n2.542912\n26.700572\n2.542912\n…\n7.628735\n6.993007\n2.542912\n2.542912\n\"BIO\"\n\n\n\n\n\n\nLet’s also examine the distribution of texts across disciplines:\n\ndiscipline_counts = df_biber.group_by(\"discipline\").len().sort(\"len\", descending=True)\nprint(\"Distribution of texts by discipline:\")\nprint(discipline_counts)\n\nDistribution of texts by discipline:\nshape: (17, 2)\n┌────────────┬─────┐\n│ discipline ┆ len │\n│ ---        ┆ --- │\n│ str        ┆ u32 │\n╞════════════╪═════╡\n│ NUR        ┆ 10  │\n│ EDU        ┆ 10  │\n│ ENG        ┆ 10  │\n│ CLS        ┆ 10  │\n│ NRE        ┆ 10  │\n│ MEC        ┆ 10  │\n│ CEE        ┆ 10  │\n│ SOC        ┆ 10  │\n│ PSY        ┆ 10  │\n│ BIO        ┆ 10  │\n│ PHY        ┆ 10  │\n│ PHI        ┆ 10  │\n│ LIN        ┆ 10  │\n│ IOE        ┆ 10  │\n│ HIS        ┆ 10  │\n│ POL        ┆ 10  │\n│ ECO        ┆ 10  │\n└────────────┴─────┘"
  },
  {
    "objectID": "biber-analyzer.html#initializing-the-biberanalyzer",
    "href": "biber-analyzer.html#initializing-the-biberanalyzer",
    "title": "Biber analyzer",
    "section": "Initializing the BiberAnalyzer",
    "text": "Initializing the BiberAnalyzer\n\nCreating the Analyzer Object\nNow we can initialize the BiberAnalyzer with our feature matrix. The id_column=True parameter indicates that our DataFrame contains both document IDs and category labels:\n\nanalyzer = pb.BiberAnalyzer(df_biber, id_column=True)\n\nThe BiberAnalyzer automatically: - Validates the input data structure - Separates numeric features from categorical variables - Computes eigenvalues for factor analysis - Performs initial data quality checks\n\n\nData Requirements and Validation\nThe BiberAnalyzer expects specific data formats: - Numeric columns: Normalized linguistic features (Float64) - String columns: Document identifiers and/or category labels - Valid grouping: Category variable should group multiple documents (not unique per document)"
  },
  {
    "objectID": "biber-analyzer.html#exploratory-data-analysis",
    "href": "biber-analyzer.html#exploratory-data-analysis",
    "title": "Biber analyzer",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDetermining Optimal Number of Factors\nBefore extracting factors, we need to determine how many factors to retain. The scree plot visualization helps identify the “elbow point” where eigenvalues level off:\n\nanalyzer.mdaviz_screeplot();\n\n\n\n\n\n\n\n\nThe scree plot shows two series: - Blue line: Eigenvalues from all features\n- Orange line: Eigenvalues after removing highly correlated features (MDA approach)\nThe MDA convention is to extract factors with eigenvalues &gt; 1.0, and to look for the point where the curve begins to level off.\n\n\nUnderstanding Feature Correlations\nThe MDA procedure retains only features that are sufficiently correlated (r &gt; 0.2) with other features, removing those that fall below this threshold. This approach leverages the natural multicollinearity among linguistic features to aggregate them into meaningful dimensions. You can examine the eigenvalue data directly:\n\nprint(\"Eigenvalues comparison:\")\nprint(analyzer.eigenvalues.head(10))\n\nEigenvalues comparison:\nshape: (10, 2)\n┌──────────┬──────────┐\n│ ev_all   ┆ ev_mda   │\n│ ---      ┆ ---      │\n│ f64      ┆ f64      │\n╞══════════╪══════════╡\n│ 9.662192 ┆ 9.655607 │\n│ 4.54516  ┆ 4.531601 │\n│ 3.00359  ┆ 2.97753  │\n│ 2.565752 ┆ 2.535722 │\n│ 2.448391 ┆ 2.429068 │\n│ 2.337111 ┆ 2.336713 │\n│ 2.197334 ┆ 2.186297 │\n│ 2.106533 ┆ 2.080074 │\n│ 1.935782 ┆ 1.915118 │\n│ 1.809263 ┆ 1.789251 │\n└──────────┴──────────┘"
  },
  {
    "objectID": "biber-analyzer.html#conducting-multi-dimensional-analysis",
    "href": "biber-analyzer.html#conducting-multi-dimensional-analysis",
    "title": "Biber analyzer",
    "section": "Conducting Multi-Dimensional Analysis",
    "text": "Conducting Multi-Dimensional Analysis\n\nFactor Extraction\nBased on the scree plot, let’s extract 3 factors (a common choice that often captures major dimensions of variation):\n\nanalyzer.mda(n_factors=3)\n\nINFO:pybiber.biber_analyzer:Dropping 2 variable(s) with max |r| &lt;= 0.20: ['f_15_gerunds', 'f_34_sentence_relatives']\n\n\nThe mda() method performs several key operations: - Retains features that are sufficiently correlated with others (correlation threshold: 0.2) - Extracts the specified number of factors using maximum likelihood estimation - Applies Promax rotation for better interpretability - Computes factor scores for each document - Calculates group means for each category\n\n\nExamining Factor Loadings and Summary Statistics\nThe MDA results provide comprehensive information about the extracted factors:\n\nanalyzer.mda_summary\n\n\nshape: (3, 6)\n\n\n\nFactor\nF\ndf\nPR(&gt;F)\nSignif\nR2\n\n\nstr\nf64\nlist[u32]\nf64\nstr\nf64\n\n\n\n\n\"factor_1\"\n3.425988\n[16, 153]\n0.000034\n\"*** p &lt; 0.001\"\n0.263771\n\n\n\"factor_2\"\n11.55233\n[16, 153]\n3.3103e-19\n\"*** p &lt; 0.001\"\n0.547119\n\n\n\"factor_3\"\n3.970458\n[16, 153]\n0.000003\n\"*** p &lt; 0.001\"\n0.293392\n\n\n\n\n\n\nThe summary table shows: - Feature loadings on each factor (values &gt; 0.35 are typically considered significant) - Communalities (proportion of variance explained for each feature) - Uniqueness (proportion of variance not explained by the factors)\n\n\nInterpreting the Factors\nTo interpret the factors, look for features with high loadings (&gt; 0.35 or &lt; -0.35). Features loading positively contribute to the positive pole of a dimension, while negative loadings contribute to the negative pole.\n\n\nVisualizing Factor Results\nThe group means plot shows how different text categories (disciplines) vary along each dimension:\n\nanalyzer.mdaviz_groupmeans(factor=2, width=2, height=5);\n\n\n\n\n\n\n\n\nThis visualization displays: - X-axis: Different discipline categories - Y-axis: Mean factor scores for each discipline - Error bars: 95% confidence intervals around the means\nCategories with higher positive scores have more features associated with the positive pole of this dimension, while negative scores indicate association with the negative pole.\n\n\nExamining Individual Document Scores\nYou can also examine the factor scores for individual documents:\n\nanalyzer.mda_dim_scores\n\n\nshape: (170, 5)\n\n\n\ndoc_id\ndoc_cat\nfactor_1\nfactor_2\nfactor_3\n\n\nstr\nstr\nf64\nf64\nf64\n\n\n\n\n\"BIO_G0_02_1\"\n\"BIO\"\n-1.047555\n2.579108\n2.741629\n\n\n\"BIO_G0_03_1\"\n\"BIO\"\n1.89817\n5.793548\n2.645765\n\n\n\"BIO_G0_06_1\"\n\"BIO\"\n2.990878\n2.540135\n3.211185\n\n\n\"BIO_G0_12_1\"\n\"BIO\"\n-3.970903\n-19.69307\n-3.853258\n\n\n\"BIO_G0_21_1\"\n\"BIO\"\n-1.683797\n-14.797897\n-6.842276\n\n\n\"BIO_G0_25_1\"\n\"BIO\"\n1.904973\n4.959696\n-2.525448\n\n\n\"BIO_G0_29_1\"\n\"BIO\"\n-3.269407\n5.220856\n6.099175\n\n\n\"BIO_G2_02_1\"\n\"BIO\"\n-2.075444\n10.794254\n6.572835\n\n\n\"BIO_G2_03_1\"\n\"BIO\"\n-11.160595\n-10.013755\n2.574394\n\n\n\"BIO_G3_02_1\"\n\"BIO\"\n-6.392135\n-2.541001\n1.353258\n\n\n…\n…\n…\n…\n…\n\n\n\"SOC_G0_01_1\"\n\"SOC\"\n-5.21524\n3.960617\n0.390415\n\n\n\"SOC_G0_02_1\"\n\"SOC\"\n0.533442\n12.483244\n3.574479\n\n\n\"SOC_G0_07_1\"\n\"SOC\"\n-10.519145\n3.482978\n-0.420552\n\n\n\"SOC_G0_13_1\"\n\"SOC\"\n29.571642\n16.727145\n-0.466964\n\n\n\"SOC_G1_01_1\"\n\"SOC\"\n-3.961574\n2.09907\n5.063635\n\n\n\"SOC_G1_08_1\"\n\"SOC\"\n-6.098901\n-7.156877\n-0.963255\n\n\n\"SOC_G1_09_1\"\n\"SOC\"\n5.832215\n-4.200802\n-1.617159\n\n\n\"SOC_G2_03_1\"\n\"SOC\"\n-1.145246\n5.567016\n0.136861\n\n\n\"SOC_G3_07_1\"\n\"SOC\"\n-8.482581\n-1.640527\n1.118681\n\n\n\"SOC_G3_08_1\"\n\"SOC\"\n-2.413674\n7.861919\n0.461343\n\n\n\n\n\n\nThese scores represent where each individual document falls along each extracted dimension. Higher positive scores indicate stronger association with features that load positively on that factor."
  },
  {
    "objectID": "biber-analyzer.html#alternative-analysis-principal-component-analysis",
    "href": "biber-analyzer.html#alternative-analysis-principal-component-analysis",
    "title": "Biber analyzer",
    "section": "Alternative Analysis: Principal Component Analysis",
    "text": "Alternative Analysis: Principal Component Analysis\n\nPCA as an Alternative Approach\nWhile MDA is the traditional approach in register analysis, Principal Component Analysis (PCA) offers an alternative dimensionality reduction method that may reveal different patterns:\n\nanalyzer.pca()\n\n\n\nPCA Visualizations\nPCA provides its own set of visualization tools. Let’s examine how groups vary along the first principal component:\n\nanalyzer.pcaviz_groupmeans(pc=1, width=6, height=3);\n\n\n\n\n\n\n\n\nWe can also examine the contribution of individual features to each principal component:\n\nanalyzer.pcaviz_contrib(pc=1, width=6, height=3);\n\n\n\n\n\n\n\n\nThis plot shows which linguistic features contribute most strongly to the first principal component, helping interpret what linguistic patterns the component captures."
  },
  {
    "objectID": "biber-analyzer.html#advanced-analysis-options",
    "href": "biber-analyzer.html#advanced-analysis-options",
    "title": "Biber analyzer",
    "section": "Advanced Analysis Options",
    "text": "Advanced Analysis Options\n\nCustomizing the Analysis\nThe BiberAnalyzer offers several customization options:\n\nAdjusting Correlation Thresholds\n\n# Use stricter correlation threshold\nanalyzer.mda(n_factors=3, cor_min=0.3)\n\n\n\nChanging Factor Loading Thresholds\n\n# Use higher threshold for significant loadings\nanalyzer.mda(n_factors=3, threshold=0.4)\n\n\n\nMultiple Scree Plot Comparisons\n\n# Compare different correlation thresholds\nanalyzer.mdaviz_screeplot(mda=False);\n\n\n\n\n\n\n\n\n\n\n\nAccessing Raw Results\nAll analysis results are stored as attributes of the analyzer object:\n\n# Check available results\navailable_results = [attr for attr in dir(analyzer) if not attr.startswith('_') and 'mda' in attr]\nprint(\"Available MDA results:\", available_results)\n\nAvailable MDA results: ['mda', 'mda_biber', 'mda_dim_scores', 'mda_group_means', 'mda_loadings', 'mda_summary', 'mdaviz_groupmeans', 'mdaviz_screeplot']"
  },
  {
    "objectID": "biber-analyzer.html#interpretation-guidelines",
    "href": "biber-analyzer.html#interpretation-guidelines",
    "title": "Biber analyzer",
    "section": "Interpretation Guidelines",
    "text": "Interpretation Guidelines\n\nReading Factor Loadings\nWhen interpreting MDA results:\n\nHigh positive loadings (&gt; 0.35): Features that increase together and contribute to the positive pole\nHigh negative loadings (&lt; -0.35): Features that contribute to the negative pole\nLow loadings (-0.35 to 0.35): Features that don’t strongly define this dimension\nCommunality: How much of each feature’s variance is explained by all factors\nUniqueness: Unexplained variance (1 - communality)\n\n\n\nComparing Text Types\nWhen comparing categories on dimensions: - Distance between groups indicates how different they are linguistically - Confidence intervals show statistical reliability of group differences\n- Consistent patterns across multiple factors suggest robust register differences"
  },
  {
    "objectID": "biber-analyzer.html#comparison-with-bibers-original-dimensions",
    "href": "biber-analyzer.html#comparison-with-bibers-original-dimensions",
    "title": "Biber analyzer",
    "section": "Comparison with Biber’s Original Dimensions",
    "text": "Comparison with Biber’s Original Dimensions\n\nProjecting onto Biber’s Factors\nOne powerful feature of the BiberAnalyzer is the ability to project your data onto Biber’s original dimensions, allowing for direct comparison with established research:\n\nanalyzer.mda_biber()\n\nThe mda_biber() method:\n\nLoads Biber’s original factor loadings from his 1988 study\nProjects your feature data onto these established dimensions\nProvides direct comparison with Biber’s dimensions:\n\nFactor 1: Involved vs. Informational Production\nFactor 2: Narrative vs. Non-narrative Concerns\n\nFactor 3: Explicit vs. Situation-dependent Reference\nFactor 4: Overt Expression of Persuasion\nFactor 5: Abstract vs. Non-abstract Information\nFactor 6: On-line Informational Elaboration\n\n\n\n\nVisualizing Biber Dimension Results\nNow we can visualize how our academic disciplines fall along Biber’s first dimension:\n\nanalyzer.mdaviz_groupmeans(factor=1, width=2, height=5);\n\n\n\n\n\n\n\n\nThis plot shows how different academic disciplines position along Biber’s first dimension (Involved vs. Informational Production). Academic texts typically fall toward the “Informational” pole (negative scores) due to their formal, expository nature."
  },
  {
    "objectID": "biber-analyzer.html#conclusion",
    "href": "biber-analyzer.html#conclusion",
    "title": "Biber analyzer",
    "section": "Conclusion",
    "text": "Conclusion\nThe BiberAnalyzer provides a comprehensive toolkit for exploring systematic patterns of linguistic variation in text corpora. By combining traditional MDA with modern computational tools and visualizations, researchers can:\n\nIdentify key dimensions of linguistic variation in their corpora\nCompare their findings with established research through Biber projection\nExplore alternative analytical approaches through PCA\nCreate publication-ready visualizations of their results\n\nThis approach enables rigorous, quantitative investigation of register variation, genre differences, and other patterns of linguistic co-occurrence in large text collections."
  }
]