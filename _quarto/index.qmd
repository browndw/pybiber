---
jupyter: python3
---

# pybiber

**A comprehensive Python package for linguistic feature extraction and Multi-Dimensional Analysis**

The pybiber package provides tools for extracting [67 lexicogrammatical and functional features](feature-categories.qmd) described by Biber [-@biber1988variation] and widely used for text-type, register, and genre classification tasks in corpus linguistics.

## Key Features

- **Feature Extraction**: Automated extraction of 67 linguistic features from text corpora
- **Multi-Dimensional Analysis**: Implementation of Biber's MDA methodology for register analysis  
- **Principal Component Analysis**: Alternative dimensionality reduction approaches
- **Visualization Tools**: Comprehensive plotting functions for exploratory data analysis
- **High Performance**: Built on [spaCy](https://spacy.io/models){.external target="_blank"} and [Polars](https://docs.pola.rs/){.external target="_blank"} for efficient processing
- **Pipeline Integration**: End-to-end workflows from raw text to statistical analysis

## Applications

The pybiber package is suitable for:

- **Register and genre analysis** in corpus linguistics
- **Text classification** and machine learning preprocessing  
- **Diachronic language change** studies
- **Cross-linguistic variation** research
- **Academic writing analysis** and pedagogical applications
- **Stylometric analysis** and authorship attribution

## Technical Foundation

The package uses [spaCy](https://spacy.io/models){.external target="_blank"} part-of-speech tagging and dependency parsing to extract linguistic features. All data processing leverages the [Polars](https://docs.pola.rs/api/python/stable/reference/index.html){.external target="_blank"} DataFrame library for high-performance analytics.

::: {.callout-warning}
## Accuracy Considerations

Feature extraction builds from the outputs of probabilistic taggers, so the accuracy of resulting counts depends on the accuracy of those models. Texts with irregular spellings, non-normative punctuation, etc. may produce unreliable outputs unless taggers are specifically tuned for those domains.
:::

## Related Implementations

- [pseudobibeR](https://cran.r-project.org/web/packages/pseudobibeR/index.html){.external target="_blank"}: R implementation of Biber's feature set
- [quanteda](https://quanteda.io/){.external target="_blank"}: R package for quantitative text analysis

---

## Quick Start

For users eager to jump in, here's a minimal example:

```{python}
#| eval: false
import pybiber as pb

# One-line corpus processing
pipeline = pb.PybiberPipeline()
features = pipeline.run_from_folder("path/to/texts")
```

---

## Installation

You can install the released version of pybiber from [PyPI](https://pypi.org/project/pybiber/){.external target="_blank"}:

```bash
pip install pybiber
```

### spaCy Model Installation

Install a [spaCy model](https://spacy.io/usage){.external target="_blank"} for linguistic analysis:

```bash
python -m spacy download en_core_web_sm
```

::: {.callout-tip}
## Alternative Installation Methods

You can also install spaCy models directly via pip:
```bash
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl
```

For other languages, see the [spaCy models documentation](https://spacy.io/models){.external target="_blank"}.
:::

## Basic Workflow

### Data Requirements

The pybiber package works with text corpora structured as DataFrames. The [](`~pybiber.parse_functions.biber`) function expects a [Polars DataFrame](https://docs.pola.rs/api/python/stable/reference/dataframe/index.html){.external target="_blank"} with:
- `doc_id` column: Unique identifiers for each document
- `text` column: Raw text content

This structure follows conventions established by [`readtext`](https://readtext.quanteda.io/articles/readtext_vignette.html){.external target="_blank"} and [quanteda](https://quanteda.io/){.external target="_blank"} in R.

### Step-by-Step Processing


#### 1. Import Libraries and Load Data

```{python}
import spacy
import pybiber as pb
from pybiber.data import micusp_mini
```

Let's examine the structure of our sample corpus:

```{python}
micusp_mini.head()
```

The `micusp_mini` dataset is a subset of the Michigan Corpus of Upper-Level Student Papers, containing academic texts from various disciplines.

::: {.callout-tip}
## Building Your Own Corpus

To process your own texts, use [](`~pybiber.parse_utils.corpus_from_folder`) to read all text files from a directory:

```python
corpus = pb.corpus_from_folder("path/to/your/texts")
```
:::

#### 2. Initialize spaCy Model

The pybiber package requires a spaCy model with part-of-speech tagging and [dependency parsing](https://spacy.io/usage/linguistic-features){.external target="_blank"} capabilities:

```{python}
#| warning: false
nlp = spacy.load("en_core_web_sm", disable=["ner"])
```

::: {.callout-note}
We disable Named Entity Recognition (`ner`) to increase processing speed, though this is optional. The essential components for feature extraction are the part-of-speech tagger and dependency parser. Note that `PybiberPipeline` disables NER by default.
:::

#### 3. Process the Corpus

Use the [](`~pybiber.parse_utils.CorpusProcessor`) to parse your texts:

```{python}
processor = pb.CorpusProcessor()
df_spacy = processor.process_corpus(micusp_mini, nlp_model=nlp)
```

This returns a token-level DataFrame with linguistic annotations, structured similarly to [spacyr](https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html){.external target="_blank"} output:

```{python}
df_spacy.head()
```

#### 4. Extract Linguistic Features

Finally, aggregate the token-level data into document-level feature counts:

```{python}
df_biber = pb.biber(df_spacy)
```

The resulting document-feature matrix contains 67 linguistic features plus document identifiers:

```{python}
print(f"Matrix dimensions: {df_biber.shape}")
df_biber.head()
```

## Understanding the Output

### Feature Normalization

::: {.callout-important}
## Feature Scaling

By default, all features are normalized per 1,000 tokens except:
- `f_43_type_token`: Type-token ratio (0-1 scale)
- `f_44_mean_word_length`: Average word length in characters

Set `normalize=False` to return absolute counts instead of normalized frequencies.
:::

### Document Metadata

::: {.callout-tip}
## Encoding Metadata

Encode important metadata into your document IDs (file names) for downstream analysis. In the `micusp_mini` data, the first three letters represent academic disciplines (e.g., BIO=Biology, ENG=English).
:::

## Advanced Usage

### High-Level Pipeline

For streamlined processing, use the [](`~pybiber.pipeline.PybiberPipeline`):

```{python}
#| eval: false
# Process a folder of .txt files in one step
pipeline = pb.PybiberPipeline(model="en_core_web_sm")
features = pipeline.run_from_folder("path/to/texts", recursive=True)
```

### Working with Pandas

All DataFrames use [Polars](https://docs.pola.rs/api/python/stable/reference/index.html){.external target="_blank"} for performance. To convert to pandas:

```{python}
#| eval: false
df_pandas = df_biber.to_pandas()  # Requires pandas and pyarrow
```

## Next Steps

- **[Get Started Guide](get-started.qmd)**: Detailed walkthrough of the basic workflow
- **[Feature Categories](feature-categories.qmd)**: Complete list and descriptions of all 67 features
- **[Biber Analyzer](biber-analyzer.qmd)**: Multi-Dimensional Analysis and statistical visualization tools

For advanced analytical workflows, explore the [`BiberAnalyzer`](biber-analyzer.qmd) class for factor analysis and dimensional visualization.
